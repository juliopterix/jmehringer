<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Julius Mehringer">
<meta name="dcterms.date" content="2025-06-13">
<meta name="description" content="Standard implementations and publically available tutorials of Hierarchical Bayesian Neural Networks (HBNNs) lack the ability to work with varying numbers of training examples per group. In this entry, I will show you how we can make use of padding and masking in order to be able to train HBNNs.">

<title>Hierarchical Bayesian Neural Networks with variable group sizes – Julius Mehringer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-7b130d57f3c2f3ea946381e377c320df.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Julius Mehringer</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/juliopterix"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-page">
      <div class="quarto-title-block"><div><h1 class="title">Hierarchical Bayesian Neural Networks with variable group sizes</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Standard implementations and publically available tutorials of Hierarchical Bayesian Neural Networks (HBNNs) lack the ability to work with varying numbers of training examples per group. In this entry, I will show you how we can make use of padding and masking in order to be able to train HBNNs.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Probabilistic Machine Learning</div>
                <div class="quarto-category">nonlinear model</div>
                <div class="quarto-category">Hierarchical Modeling</div>
                <div class="quarto-category">jax</div>
                <div class="quarto-category">blackjax</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta column-page">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p><a href="https://github.com/juliopterix">Julius Mehringer</a> <a href="https://orcid.org/0009-0006-2827-9156" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a></p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 13, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block column-page" id="quarto-document-content">





<p>Bayesian Modeling is a very suitable choice if you want to obtain the uncertainty associated with the predictions of a model. Here, typically a Markov Chain Monte Carlo estimator is used, which explores any stationary distribution and recovers (asymptotically) consistent estimators and thus those samplers are of primary interest, because we can essentially (re-)construct any distribution. This can also be the joint distribution of the weights of a Neural Network! This could be incredibly promising, since we can combine the powers from statistical modeling techniques with the of universal function approximation from Neural Networks. To this end, there are recent <a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/papamarkou24b/papamarkou24b.pdf">voices</a> arguing why <mark>Bayesian Deep Learning is a promising avenue</mark>.</p>
<p>In Bayesian Modeling, <em>Hierarchical Bayesian Modeling</em> is a special kind of model specification, helping the sampler to expore the distribution of interest. It is in fact so powerful that once you know about it, you can’t unsee applications of it (primarily in the Sciences). Hierarchical Modeling can be used if you have some <strong>grouped</strong> structure in your dataset, e.g.&nbsp;if products can be assigned to clusters that share some properties. More on this technique in <a href="#sec-hbnns" class="quarto-xref">Section&nbsp;2</a>.</p>
<p>There are some very useful blog entries and notebooks out there (e.g.&nbsp;<a href="https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/">by Thomas Wiecki</a> using Theano and PyMC3 and <a href="https://github.com/homerjed/hbnn">this repo</a> using a more recent version of JAX). However, those examples only work with the critical assumption that the group sizes are all <strong>of the same size</strong>. In reality, this is rarely the case, of course.</p>
<p>Here, I will show you how you can implement a <code>Hierarchical Bayesian Neural Network</code> irrespective of the group sizes you observe in your dataset.</p>
<p>The notebook is structured as follows:</p>
<ol type="1">
<li>✍️ Create a dataset: Binary classification with unequal observations per group</li>
<li>🧠 What’s the modelling approach and why does it work?</li>
<li>👾 Code the model</li>
<li>🚀 Run the model and evaluate</li>
</ol>
<section id="setup-and-dummy-data-generation" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Setup and dummy data generation</h1>
<p>Let’s first import the libraries we’ll need:</p>
<div id="load-libraries" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> date</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> warnings <span class="im">import</span> filterwarnings</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.random <span class="im">as</span> jr</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> scale</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> blackjax</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability.substrates.jax.distributions <span class="im">as</span> tfd</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jr.key(<span class="bu">int</span>(date.today().strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">"</span>)))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> mpl.colormaps[<span class="st">"RdYlGn"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Thoughout the notebook, we’ll use the standard <code>two-moons</code> dataset, being a binary classification problem. <a href="#fig-two-moons" class="quarto-xref">Figure&nbsp;1</a> shows how the dataset looks with some training examples.</p>
<div id="cell-fig-two-moons" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> make_moons(noise<span class="op">=</span>noise, n_samples<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[Y <span class="op">==</span> i, <span class="dv">0</span>], X[Y <span class="op">==</span> i, <span class="dv">1</span>], color<span class="op">=</span>cmap(<span class="bu">float</span>(i)), label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-two-moons" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-two-moons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-two-moons-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-two-moons-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: The standard two-moons dataset plot
</figcaption>
</figure>
</div>
</div>
</div>
<p>Next, let’s choose some values for the data generation of our grouped dataset. We’ll create several groups with a random number of samples, choose some settings for our Neural Network implementation and set two parameters for the MCMC-Algorithm: the number of ‘warmup’ samples (which will be discarded after the model fitting finished) and the number of sampling steps.</p>
<div id="data-and-model-settings" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>n_groups <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>n_grps_sq <span class="op">=</span> <span class="bu">int</span>(np.sqrt(n_groups))</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> np.random.randint(<span class="dv">10</span>, <span class="dv">200</span>, size<span class="op">=</span>n_groups)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># MLP params</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>data_dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>hidden_layer_width <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>n_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sampling</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>num_warmup <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">2000</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We then write a function which rotates the dataset in the 2-D space a bit and generate the datasets, store them in lists:</p>
<div id="generate-groups" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rotate(X, deg):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.radians(deg)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    c, s <span class="op">=</span> np.cos(theta), np.sin(theta)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.matrix([[c, <span class="op">-</span>s], [s, c]])</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.dot(R)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(X)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">31</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>Xs, Ys, gs <span class="op">=</span> [], [], []</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>Xs_train, Ys_train, gs_train, Xs_test, Ys_test, gs_test <span class="op">=</span> [], [], [], [], [], []</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_groups):</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate data with 2 classes that are not linearly separable</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> make_moons(noise<span class="op">=</span>noise, n_samples<span class="op">=</span>n_samples[i])</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> scale(X)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rotate the points randomly for each category</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    rotate_by <span class="op">=</span> np.random.randn() <span class="op">*</span> <span class="fl">90.0</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> rotate(X, rotate_by)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    Xs.append(X)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    Ys.append(Y)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    gs.append(X.shape[<span class="dv">0</span>])</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">31</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    Xs_train.append(X_train)</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    Ys_train.append(Y_train)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    gs_train.append(X_train.shape[<span class="dv">0</span>])</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    Xs_test.append(X_test)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    Ys_test.append(Y_test)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    gs_test.append(X_test.shape[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we pad the entries in our list of datasets such that all the entries have the same shape: the shape of the largest dataset. We also create a mask, marking the elements of the entries which were padded. Padding works here, because we can disregard the masked positions in our datasets in the <code>loglikelihood function</code>.</p>
<div id="apply-padding" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pad_arrays(arrays, fill_value):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    max_size <span class="op">=</span> <span class="bu">max</span>(array.shape[<span class="dv">0</span>] <span class="cf">for</span> array <span class="kw">in</span> arrays)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    padded_arrays <span class="op">=</span> []</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> array <span class="kw">in</span> arrays:</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> array.ndim <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>            padding <span class="op">=</span> (<span class="dv">0</span>, max_size <span class="op">-</span> array.shape[<span class="dv">0</span>]) </span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>            padded_array <span class="op">=</span> jnp.pad(array, padding, mode<span class="op">=</span><span class="st">"constant"</span>, constant_values<span class="op">=</span>fill_value)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            padded_arrays.append(padded_array[:, np.newaxis])  </span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>            padding <span class="op">=</span> ((<span class="dv">0</span>, max_size <span class="op">-</span> array.shape[<span class="dv">0</span>]), (<span class="dv">0</span>, <span class="dv">0</span>))  </span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>            padded_array <span class="op">=</span> jnp.pad(array, padding, mode<span class="op">=</span><span class="st">"constant"</span>, constant_values<span class="op">=</span>fill_value)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>            padded_arrays.append(padded_array)</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> padded_arrays</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Stack group arrays and create a mask</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>fill_value <span class="op">=</span> <span class="fl">1e5</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>Xs_train <span class="op">=</span> jnp.stack(pad_arrays(Xs_train, fill_value))</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>Ys_train <span class="op">=</span> jnp.stack(pad_arrays(Ys_train, fill_value)).squeeze(axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>Xs_test <span class="op">=</span> jnp.stack(pad_arrays(Xs_test, fill_value))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>Ys_test <span class="op">=</span> jnp.stack(pad_arrays(Ys_test, fill_value)).squeeze(axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>mask_train <span class="op">=</span> jnp.where(Xs_train <span class="op">==</span> fill_value, fill_value, <span class="dv">1</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>mask_test <span class="op">=</span> jnp.where(Xs_test <span class="op">==</span> fill_value, fill_value, <span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In <a href="#fig-show-datasets" class="quarto-xref">Figure&nbsp;2</a> you can see how the datasets look and how many entries the individual groups got:</p>
<div id="cell-fig-show-datasets" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># utility function for plotting</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> closest_factors(n):</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> <span class="dv">1</span>, n</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">int</span>(n<span class="op">**</span><span class="fl">0.5</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">%</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>            a, b <span class="op">=</span> i, n <span class="op">//</span> i </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a, b</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of rows and columns for subplots</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>n_cols, n_rows <span class="op">=</span> closest_factors(n_groups)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op">=</span>(n_cols<span class="op">*</span><span class="dv">3</span>, n_rows<span class="op">*</span><span class="dv">2</span>), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten axes array for easy iteration</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (X, Y, group_size, ax) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(Xs_train, Ys_train, gs_train, axes)):</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[Y <span class="op">==</span> c, <span class="dv">0</span>], X[Y <span class="op">==</span> c, <span class="dv">1</span>], color<span class="op">=</span>cmap(<span class="bu">float</span>(c)), label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([])</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([])</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    ax.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(title<span class="op">=</span><span class="ss">f"Category </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, N_training = </span><span class="sc">{</span>group_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Hide any unused subplots</span></span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_groups, <span class="bu">len</span>(axes)):</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    fig.delaxes(axes[j])</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-show-datasets" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-show-datasets-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="index_files/figure-html/fig-show-datasets-output-1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-show-datasets-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Groups with varying numbers of training examples
</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-hbnns" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> A Bayesian Hierarchical Neural Network</h1>
<p>Now, let’s dig into why modeling this dataset with a <code>hierarchical</code> Neural Network might make sense. Beware: we’ll use some vocabulary from statistical modeling.</p>
<p>The prominent probabilistic modeler <a href="https://betanalpha.github.io">Michael Betancourt</a> provides an <a href="https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html">in depth introduction to the foundations of hierarchical modeling</a>. Conceptually, hierarchical modeling is an approach if there is a latent population that we can couple context-dependent parameters to. In our dataset, we assume there is some homogeneous structure <em>withtin</em> the groups, whereas the groups may be different <em>between</em> them. This means that we can actually share the weights of the group’s individual Neural Networks <strong>across</strong> all networks, because the task (binary classification with some Z-shape) is similar; even though the individual groups are all oriented differently in the 2-D space (difference between the groups).</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
When does a Hierarchical Bayesian Neural Network make the most sense?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Summarizing, a HBNN requires that:</p>
<ul>
<li>you can make use of a grouping structure in your dataset</li>
<li>the data generating process of the individual groups is similar</li>
</ul>
<p>It is the strongest, if you have a small-ish number of observations (probably in relation to the difficulty of the learning task?), since in this case ‘traditional’ approaches will fail (see <a href="#tip-book-example" class="quarto-xref">Tip&nbsp;1</a>).</p>
</div>
</div>
<div id="tip-book-example" class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip&nbsp;1: Recap: Running a separate model per group (example from the Blackjax Sampling book)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Since it’s covered in the <a href="https://blackjax-devs.github.io/sampling-book/models/hierarchical_bnn.html">HBNN-tutorial from the BlackJax Sampling Book</a>, I will just copy-paste the code here for reference and the curious (and because I don’t want to loose this example if the Sampling Book disappears at some point).</p>
<p><strong>The key takeaway is this</strong>: If you fit the models separately, there will be not enough training examples for the Neural Network to capture the nonlinear relationship separating the two classes.</p>
<div id="cell-recap-sampling-book-example" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> date</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> warnings <span class="im">import</span> filterwarnings</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax <span class="im">import</span> linen <span class="im">as</span> nn</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax.linen.initializers <span class="im">import</span> ones</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability.substrates.jax.distributions <span class="im">as</span> tfd</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> scale</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> blackjax</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"axes.spines.right"</span>] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"axes.spines.top"</span>] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>rng_key <span class="op">=</span> jax.random.key(<span class="bu">int</span>(date.today().strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">"</span>)))</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>n_groups_tut <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>n_grps_sq_tut <span class="op">=</span> <span class="bu">int</span>(np.sqrt(n_groups_tut))</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>n_samples_tut <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rotate_tut(X, deg):</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.radians(deg)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    c, s <span class="op">=</span> np.cos(theta), np.sin(theta)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.matrix([[c, <span class="op">-</span>s], [s, c]])</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.dot(R)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(X)</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">31</span>)</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>Xs_tut, Ys_tut <span class="op">=</span> [], []</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_groups_tut):</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate data with 2 classes that are not linearly separable</span></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> make_moons(noise<span class="op">=</span><span class="fl">0.3</span>, n_samples<span class="op">=</span>n_samples_tut)</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> scale(X)</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rotate the points randomly for each category</span></span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    rotate_by <span class="op">=</span> np.random.randn() <span class="op">*</span> <span class="fl">90.0</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> rotate_tut(X, rotate_by)</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>    Xs_tut.append(X)</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>    Ys_tut.append(Y)</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>Xs_tut <span class="op">=</span> jnp.stack(Xs_tut)</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>Ys_tut <span class="op">=</span> jnp.stack(Ys_tut)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>Xs_tut_train <span class="op">=</span> Xs_tut[:, : n_samples_tut <span class="op">//</span> <span class="dv">2</span>, :]</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>Xs_tut_test <span class="op">=</span> Xs_tut[:, n_samples_tut <span class="op">//</span> <span class="dv">2</span> :, :]</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>Ys_tut_train <span class="op">=</span> Ys_tut[:, : n_samples_tut <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>Ys_tut_test <span class="op">=</span> Ys_tut[:, n_samples_tut <span class="op">//</span> <span class="dv">2</span> :]</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> jnp.mgrid[<span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>, <span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>].reshape((<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>grid_3d <span class="op">=</span> jnp.repeat(grid[<span class="va">None</span>, ...], n_groups_tut, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inference_loop_tut(rng_key, step_fn, initial_state, num_samples):</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> one_step(state, rng_key):</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a>        state, _ <span class="op">=</span> step_fn(rng_key, state)</span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state, state</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> jax.random.split(rng_key, num_samples)</span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>    _, states <span class="op">=</span> jax.lax.scan(one_step, initial_state, keys)</span>
<span id="cb7-75"><a href="#cb7-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-76"><a href="#cb7-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> states</span>
<span id="cb7-77"><a href="#cb7-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-78"><a href="#cb7-78" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_predictions_tut(model, samples, X, rng_key):</span>
<span id="cb7-79"><a href="#cb7-79" aria-hidden="true" tabindex="-1"></a>    vectorized_apply <span class="op">=</span> jax.vmap(model.<span class="bu">apply</span>, in_axes<span class="op">=</span>(<span class="dv">0</span>, <span class="va">None</span>), out_axes<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-80"><a href="#cb7-80" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> vectorized_apply(samples, X)</span>
<span id="cb7-81"><a href="#cb7-81" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> tfd.Bernoulli(logits<span class="op">=</span>z).sample(seed<span class="op">=</span>rng_key)</span>
<span id="cb7-82"><a href="#cb7-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-83"><a href="#cb7-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb7-84"><a href="#cb7-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-85"><a href="#cb7-85" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_mean_predictions_tut(predictions, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb7-86"><a href="#cb7-86" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute mean prediction and confidence interval around median</span></span>
<span id="cb7-87"><a href="#cb7-87" aria-hidden="true" tabindex="-1"></a>    mean_prediction <span class="op">=</span> jnp.mean(predictions, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-88"><a href="#cb7-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_prediction <span class="op">&gt;</span> threshold</span>
<span id="cb7-89"><a href="#cb7-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-90"><a href="#cb7-90" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_and_eval_tut(</span>
<span id="cb7-91"><a href="#cb7-91" aria-hidden="true" tabindex="-1"></a>    rng_key,</span>
<span id="cb7-92"><a href="#cb7-92" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb7-93"><a href="#cb7-93" aria-hidden="true" tabindex="-1"></a>    logdensity_fn,</span>
<span id="cb7-94"><a href="#cb7-94" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb7-95"><a href="#cb7-95" aria-hidden="true" tabindex="-1"></a>    Y_train,</span>
<span id="cb7-96"><a href="#cb7-96" aria-hidden="true" tabindex="-1"></a>    X_test,</span>
<span id="cb7-97"><a href="#cb7-97" aria-hidden="true" tabindex="-1"></a>    grid,</span>
<span id="cb7-98"><a href="#cb7-98" aria-hidden="true" tabindex="-1"></a>    n_groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb7-99"><a href="#cb7-99" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb7-100"><a href="#cb7-100" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb7-101"><a href="#cb7-101" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb7-102"><a href="#cb7-102" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb7-103"><a href="#cb7-103" aria-hidden="true" tabindex="-1"></a>        init_key,</span>
<span id="cb7-104"><a href="#cb7-104" aria-hidden="true" tabindex="-1"></a>        warmup_key,</span>
<span id="cb7-105"><a href="#cb7-105" aria-hidden="true" tabindex="-1"></a>        inference_key,</span>
<span id="cb7-106"><a href="#cb7-106" aria-hidden="true" tabindex="-1"></a>        train_key,</span>
<span id="cb7-107"><a href="#cb7-107" aria-hidden="true" tabindex="-1"></a>        test_key,</span>
<span id="cb7-108"><a href="#cb7-108" aria-hidden="true" tabindex="-1"></a>        grid_key,</span>
<span id="cb7-109"><a href="#cb7-109" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">=</span> jax.random.split(rng_key, <span class="dv">6</span>)</span>
<span id="cb7-110"><a href="#cb7-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-111"><a href="#cb7-111" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_groups <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-112"><a href="#cb7-112" aria-hidden="true" tabindex="-1"></a>        initial_position <span class="op">=</span> model.init(init_key, jnp.ones(X_train.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb7-113"><a href="#cb7-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-114"><a href="#cb7-114" aria-hidden="true" tabindex="-1"></a>        initial_position <span class="op">=</span> model.init(init_key, jnp.ones(X_train.shape))</span>
<span id="cb7-115"><a href="#cb7-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-116"><a href="#cb7-116" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialization</span></span>
<span id="cb7-117"><a href="#cb7-117" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> partial(logdensity_fn, X<span class="op">=</span>X_train, Y<span class="op">=</span>Y_train, model<span class="op">=</span>model)</span>
<span id="cb7-118"><a href="#cb7-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-119"><a href="#cb7-119" aria-hidden="true" tabindex="-1"></a>    <span class="co"># warm up</span></span>
<span id="cb7-120"><a href="#cb7-120" aria-hidden="true" tabindex="-1"></a>    adapt <span class="op">=</span> blackjax.window_adaptation(blackjax.nuts, logprob)</span>
<span id="cb7-121"><a href="#cb7-121" aria-hidden="true" tabindex="-1"></a>    (final_state, params), _ <span class="op">=</span> adapt.run(warmup_key, initial_position, num_warmup)</span>
<span id="cb7-122"><a href="#cb7-122" aria-hidden="true" tabindex="-1"></a>    step_fn <span class="op">=</span> blackjax.nuts(logprob, <span class="op">**</span>params).step</span>
<span id="cb7-123"><a href="#cb7-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-124"><a href="#cb7-124" aria-hidden="true" tabindex="-1"></a>    <span class="co"># inference</span></span>
<span id="cb7-125"><a href="#cb7-125" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> inference_loop_tut(inference_key, step_fn, final_state, num_samples)</span>
<span id="cb7-126"><a href="#cb7-126" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> states.position</span>
<span id="cb7-127"><a href="#cb7-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-128"><a href="#cb7-128" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluation</span></span>
<span id="cb7-129"><a href="#cb7-129" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions_tut(model, samples, X_train, train_key)</span>
<span id="cb7-130"><a href="#cb7-130" aria-hidden="true" tabindex="-1"></a>    Y_pred_train <span class="op">=</span> get_mean_predictions_tut(predictions)</span>
<span id="cb7-131"><a href="#cb7-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-132"><a href="#cb7-132" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions_tut(model, samples, X_test, test_key)</span>
<span id="cb7-133"><a href="#cb7-133" aria-hidden="true" tabindex="-1"></a>    Y_pred_test <span class="op">=</span> get_mean_predictions_tut(predictions)</span>
<span id="cb7-134"><a href="#cb7-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-135"><a href="#cb7-135" aria-hidden="true" tabindex="-1"></a>    pred_grid <span class="op">=</span> get_predictions_tut(model, samples, grid, grid_key)</span>
<span id="cb7-136"><a href="#cb7-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-137"><a href="#cb7-137" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y_pred_train, Y_pred_test, pred_grid</span>
<span id="cb7-138"><a href="#cb7-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-139"><a href="#cb7-139" aria-hidden="true" tabindex="-1"></a><span class="co"># MLP params</span></span>
<span id="cb7-140"><a href="#cb7-140" aria-hidden="true" tabindex="-1"></a>hidden_layer_width_tut <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-141"><a href="#cb7-141" aria-hidden="true" tabindex="-1"></a>n_hidden_layers_tut <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb7-142"><a href="#cb7-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-143"><a href="#cb7-143" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NN(nn.Module):</span>
<span id="cb7-144"><a href="#cb7-144" aria-hidden="true" tabindex="-1"></a>    n_hidden_layers: <span class="bu">int</span></span>
<span id="cb7-145"><a href="#cb7-145" aria-hidden="true" tabindex="-1"></a>    layer_width: <span class="bu">int</span></span>
<span id="cb7-146"><a href="#cb7-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-147"><a href="#cb7-147" aria-hidden="true" tabindex="-1"></a>    <span class="at">@nn.compact</span></span>
<span id="cb7-148"><a href="#cb7-148" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb7-149"><a href="#cb7-149" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_hidden_layers):</span>
<span id="cb7-150"><a href="#cb7-150" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> nn.Dense(features<span class="op">=</span><span class="va">self</span>.layer_width)(x)</span>
<span id="cb7-151"><a href="#cb7-151" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> nn.tanh(x)</span>
<span id="cb7-152"><a href="#cb7-152" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> nn.Dense(features<span class="op">=</span><span class="dv">1</span>)(x)</span>
<span id="cb7-153"><a href="#cb7-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-154"><a href="#cb7-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-155"><a href="#cb7-155" aria-hidden="true" tabindex="-1"></a>bnn <span class="op">=</span> NN(n_hidden_layers_tut, hidden_layer_width_tut)</span>
<span id="cb7-156"><a href="#cb7-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-157"><a href="#cb7-157" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logprior_fn_tut(params):</span>
<span id="cb7-158"><a href="#cb7-158" aria-hidden="true" tabindex="-1"></a>    leaves, _ <span class="op">=</span> jax.tree_util.tree_flatten(params)</span>
<span id="cb7-159"><a href="#cb7-159" aria-hidden="true" tabindex="-1"></a>    flat_params <span class="op">=</span> jnp.concatenate([jnp.ravel(a) <span class="cf">for</span> a <span class="kw">in</span> leaves])</span>
<span id="cb7-160"><a href="#cb7-160" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(tfd.Normal(<span class="dv">0</span>, <span class="dv">1</span>).log_prob(flat_params))</span>
<span id="cb7-161"><a href="#cb7-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-162"><a href="#cb7-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-163"><a href="#cb7-163" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loglikelihood_fn_tut(params, X, Y, model):</span>
<span id="cb7-164"><a href="#cb7-164" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> jnp.ravel(model.<span class="bu">apply</span>(params, X))</span>
<span id="cb7-165"><a href="#cb7-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(tfd.Bernoulli(logits).log_prob(Y))</span>
<span id="cb7-166"><a href="#cb7-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-167"><a href="#cb7-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-168"><a href="#cb7-168" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logdensity_fn_of_bnn_tut(params, X, Y, model):</span>
<span id="cb7-169"><a href="#cb7-169" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logprior_fn_tut(params) <span class="op">+</span> loglikelihood_fn_tut(params, X, Y, model)</span>
<span id="cb7-170"><a href="#cb7-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-171"><a href="#cb7-171" aria-hidden="true" tabindex="-1"></a>rng_key, eval_key <span class="op">=</span> jax.random.split(rng_key)</span>
<span id="cb7-172"><a href="#cb7-172" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> jax.random.split(eval_key, n_groups_tut)</span>
<span id="cb7-173"><a href="#cb7-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-174"><a href="#cb7-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-175"><a href="#cb7-175" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_and_eval_single_mlp_tut(key, X_train, Y_train, X_test):</span>
<span id="cb7-176"><a href="#cb7-176" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fit_and_eval_tut(</span>
<span id="cb7-177"><a href="#cb7-177" aria-hidden="true" tabindex="-1"></a>        key, bnn, logdensity_fn_of_bnn_tut, X_train, Y_train, X_test, grid, n_groups<span class="op">=</span><span class="va">None</span></span>
<span id="cb7-178"><a href="#cb7-178" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-179"><a href="#cb7-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-180"><a href="#cb7-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-181"><a href="#cb7-181" aria-hidden="true" tabindex="-1"></a>Ys_pred_train, Ys_pred_test, ppc_grid_single <span class="op">=</span> jax.vmap(fit_and_eval_single_mlp_tut)(</span>
<span id="cb7-182"><a href="#cb7-182" aria-hidden="true" tabindex="-1"></a>    keys, Xs_tut_train, Ys_tut_train, Xs_tut_test</span>
<span id="cb7-183"><a href="#cb7-183" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-184"><a href="#cb7-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-185"><a href="#cb7-185" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_surfaces_non_hierarchical_tut(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb7-186"><a href="#cb7-186" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(</span>
<span id="cb7-187"><a href="#cb7-187" aria-hidden="true" tabindex="-1"></a>        figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">12</span>), nrows<span class="op">=</span>nrows, ncols<span class="op">=</span>ncols, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span></span>
<span id="cb7-188"><a href="#cb7-188" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-189"><a href="#cb7-189" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> axes.flatten()</span>
<span id="cb7-190"><a href="#cb7-190" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, Y_pred, Y_true, ax) <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb7-191"><a href="#cb7-191" aria-hidden="true" tabindex="-1"></a>        <span class="bu">zip</span>(Xs_tut_train, Ys_pred_train, Ys_tut_train, axes)</span>
<span id="cb7-192"><a href="#cb7-192" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb7-193"><a href="#cb7-193" aria-hidden="true" tabindex="-1"></a>        ax.contourf(</span>
<span id="cb7-194"><a href="#cb7-194" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">0</span>].reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb7-195"><a href="#cb7-195" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">1</span>].reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb7-196"><a href="#cb7-196" aria-hidden="true" tabindex="-1"></a>            ppc_grid_single[i, ...].mean(axis<span class="op">=</span><span class="dv">0</span>).reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb7-197"><a href="#cb7-197" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>cmap,</span>
<span id="cb7-198"><a href="#cb7-198" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-199"><a href="#cb7-199" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb7-200"><a href="#cb7-200" aria-hidden="true" tabindex="-1"></a>            ax.scatter(</span>
<span id="cb7-201"><a href="#cb7-201" aria-hidden="true" tabindex="-1"></a>                X[Y_true <span class="op">==</span> i, <span class="dv">0</span>], X[Y_true <span class="op">==</span> i, <span class="dv">1</span>], </span>
<span id="cb7-202"><a href="#cb7-202" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span>cmap(<span class="bu">float</span>(i)), label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, alpha<span class="op">=</span><span class="fl">.8</span>)</span>
<span id="cb7-203"><a href="#cb7-203" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb7-204"><a href="#cb7-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-205"><a href="#cb7-205" aria-hidden="true" tabindex="-1"></a>plot_decision_surfaces_non_hierarchical_tut(nrows<span class="op">=</span>n_grps_sq_tut, ncols<span class="op">=</span>n_grps_sq_tut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/recap-sampling-book-example-output-1.png" id="recap-sampling-book-example" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section>
<section id="coding-the-model" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Coding the model</h1>
<p>Now we’re ready for the modeling code. To get an overview of the model we we’re about to create, have a look at <a href="#fig-model-depict" class="quarto-xref">Figure&nbsp;3</a>. There, we have <strong>groups</strong> <span class="math inline">\(g=1:G\)</span> with the respective weight matrices <span class="math inline">\(w^g_{l}\)</span> for the input, hidden and output layers <span class="math inline">\(l\)</span>.</p>
<p>The group weights are drawn from a Normal distribution, which, in a <em>non-centered</em> form means</p>
<p><span class="math display">\[
w^g_l = \mu_{l} + \epsilon^g_{l} \sigma_l
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\begin{align}\mu_l &amp;\sim \mathcal{N}(0,1) \\\epsilon^g_l &amp;\sim \mathcal{N}(0,1) \\\sigma_l &amp;= 1\end{align}
\]</span></p>
<p>This non-centered formulation simplifies the space to be explored by our sampler.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Centered Formulation
</div>
</div>
<div class="callout-body-container callout-body">
<p>We don’t model the individual weights directly. This ‘Centered Formulation’ would mean <span class="math inline">\(w^g_l \sim \mathcal{N}(\mu_l, \sigma_l)\)</span> with usually a <span class="math inline">\(\mathcal{N}\)</span> prior for <span class="math inline">\(\mu\)</span>; and a <span class="math inline">\(\mathcal{N}^+\)</span> prior for <span class="math inline">\(\sigma\)</span>.</p>
</div>
</div>
<div id="fig-model-depict" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-model-depict-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="figures/model_composition.svg" class="img-fluid figure-img" style="width:70.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-model-depict-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Graphical Depiction of the model we want to train (slightly adapted from <a href="https://blackjax-devs.github.io/sampling-book/models/hierarchical_bnn.html">the HBNN sampling book reference</a>)
</figcaption>
</figure>
</div>
<p>In the following code block, we write the model using <a href="https://docs.kidger.site/equinox/all-of-equinox/">Equinox</a>, which in turn uses <a href="https://github.com/jax-ml/jax">JAX</a> for all the numerical routines (e.g.&nbsp;autodiff).</p>
<div id="code-hnn" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="annotated-cell-7"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="1">1</button><span id="annotated-cell-7-1" class="code-annotation-target"><a href="#annotated-cell-7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NonCentredLinear(eqx.Module):</span>
<span id="annotated-cell-7-2"><a href="#annotated-cell-7-2" aria-hidden="true" tabindex="-1"></a>    mu: jax.Array</span>
<span id="annotated-cell-7-3"><a href="#annotated-cell-7-3" aria-hidden="true" tabindex="-1"></a>    eps: jax.Array</span>
<span id="annotated-cell-7-4"><a href="#annotated-cell-7-4" aria-hidden="true" tabindex="-1"></a>    std: jax.Array</span>
<span id="annotated-cell-7-5"><a href="#annotated-cell-7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-6"><a href="#annotated-cell-7-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_size, out_size, n_groups, <span class="op">*</span>, key):</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="2">2</button><span id="annotated-cell-7-7" class="code-annotation-target"><a href="#annotated-cell-7-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu <span class="op">=</span> jr.normal(key, (in_size, out_size))</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="3">3</button><span id="annotated-cell-7-8" class="code-annotation-target"><a href="#annotated-cell-7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> jr.normal(key, (n_groups, in_size, out_size))</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="4">4</button><span id="annotated-cell-7-9" class="code-annotation-target"><a href="#annotated-cell-7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.std <span class="op">=</span> jnp.ones((<span class="dv">1</span>,))</span>
<span id="annotated-cell-7-10"><a href="#annotated-cell-7-10" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="5">5</button><span id="annotated-cell-7-11" class="code-annotation-target"><a href="#annotated-cell-7-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="annotated-cell-7-12"><a href="#annotated-cell-7-12" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.mu <span class="op">+</span> <span class="va">self</span>.eps <span class="op">*</span> <span class="va">self</span>.std</span>
<span id="annotated-cell-7-13"><a href="#annotated-cell-7-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">@</span> w</span>
<span id="annotated-cell-7-14"><a href="#annotated-cell-7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-15"><a href="#annotated-cell-7-15" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="6">6</button><span id="annotated-cell-7-16" class="code-annotation-target"><a href="#annotated-cell-7-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HNN(eqx.Module):</span>
<span id="annotated-cell-7-17"><a href="#annotated-cell-7-17" aria-hidden="true" tabindex="-1"></a>    layers: Tuple[NonCentredLinear]</span>
<span id="annotated-cell-7-18"><a href="#annotated-cell-7-18" aria-hidden="true" tabindex="-1"></a>    out: eqx.nn.Linear</span>
<span id="annotated-cell-7-19"><a href="#annotated-cell-7-19" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="7">7</button><span id="annotated-cell-7-20" class="code-annotation-target"><a href="#annotated-cell-7-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data_dim, layer_width, n_layers, n_groups, <span class="op">*</span>, key):</span>
<span id="annotated-cell-7-21"><a href="#annotated-cell-7-21" aria-hidden="true" tabindex="-1"></a>        dims <span class="op">=</span> [data_dim] <span class="op">+</span> [layer_width] <span class="op">*</span> n_layers</span>
<span id="annotated-cell-7-22"><a href="#annotated-cell-7-22" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="annotated-cell-7-23"><a href="#annotated-cell-7-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n, (_in, _out) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(dims[:<span class="op">-</span><span class="dv">1</span>], dims[<span class="dv">1</span>:])):</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="8">8</button><span id="annotated-cell-7-24" class="code-annotation-target"><a href="#annotated-cell-7-24" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> NonCentredLinear(_in, _out, n_groups, key<span class="op">=</span>jr.fold_in(key, n))</span>
<span id="annotated-cell-7-25"><a href="#annotated-cell-7-25" aria-hidden="true" tabindex="-1"></a>            layers <span class="op">+=</span> [layer]</span>
<span id="annotated-cell-7-26"><a href="#annotated-cell-7-26" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> <span class="bu">tuple</span>(layers)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="9">9</button><span id="annotated-cell-7-27" class="code-annotation-target"><a href="#annotated-cell-7-27" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> eqx.nn.Linear(layer_width, <span class="dv">1</span>, key<span class="op">=</span>key)</span>
<span id="annotated-cell-7-28"><a href="#annotated-cell-7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-7-29"><a href="#annotated-cell-7-29" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="annotated-cell-7-30"><a href="#annotated-cell-7-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="annotated-cell-7-31"><a href="#annotated-cell-7-31" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-7" data-target-annotation="10">10</button><span id="annotated-cell-7-32" class="code-annotation-target"><a href="#annotated-cell-7-32" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> jax.nn.tanh(x)</span>
<span id="annotated-cell-7-33"><a href="#annotated-cell-7-33" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Vmap over groups and samples</span></span>
<span id="annotated-cell-7-34"><a href="#annotated-cell-7-34" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> jax.vmap(jax.vmap(<span class="va">self</span>.out))(x)</span>
<span id="annotated-cell-7-35"><a href="#annotated-cell-7-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> o</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-7" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="1" data-code-annotation="1">Write the Non-centered layers as Equinox module</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="7" data-code-annotation="2">Initialize the weights in <span class="math inline">\(\mu\)</span> as standard Normal, one for each layer</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="3">3</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="8" data-code-annotation="3">Initialize the weights in <span class="math inline">\(\epsilon\)</span> as standard Normal, one for each layer <strong>and</strong> group</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="4">4</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="9" data-code-annotation="4">Initialize <span class="math inline">\(\sigma\)</span> as 1.</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="5">5</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="11,12,13" data-code-annotation="5">Non-centered combination of the matrices and the dot-product of <span class="math inline">\(x\)</span> and <span class="math inline">\(w\)</span>.</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="6">6</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="16" data-code-annotation="6">Write the Hierarchical Neural Network as Equinox module</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="7">7</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="20" data-code-annotation="7">In this implementation, all layers have the same width</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="8">8</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="24" data-code-annotation="8">Create all hidden layers</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="9">9</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="27" data-code-annotation="9">Final linear layer</span>
</dd>
<dt data-target-cell="annotated-cell-7" data-target-annotation="10">10</dt>
<dd>
<span data-code-cell="annotated-cell-7" data-code-lines="32" data-code-annotation="10">Choose <span class="math inline">\(\tanh\)</span> as activation function</span>
</dd>
</dl>
</div>
</div>
<p>Next, we instantiate the HNN model and write some code that Equinox needs.</p>
<div id="instantiate-model" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="annotated-cell-8"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-8-1"><a href="#annotated-cell-8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_init_apply_fns(model):</span>
<span id="annotated-cell-8-2"><a href="#annotated-cell-8-2" aria-hidden="true" tabindex="-1"></a>    params, static <span class="op">=</span> eqx.partition(model, eqx.is_inexact_array)</span>
<span id="annotated-cell-8-3"><a href="#annotated-cell-8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-8-4"><a href="#annotated-cell-8-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_fn():</span>
<span id="annotated-cell-8-5"><a href="#annotated-cell-8-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> params</span>
<span id="annotated-cell-8-6"><a href="#annotated-cell-8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-8-7"><a href="#annotated-cell-8-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> apply_fn(_params, x):</span>
<span id="annotated-cell-8-8"><a href="#annotated-cell-8-8" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> eqx.combine(_params, static)</span>
<span id="annotated-cell-8-9"><a href="#annotated-cell-8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model(x)</span>
<span id="annotated-cell-8-10"><a href="#annotated-cell-8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-8-11"><a href="#annotated-cell-8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> init_fn, apply_fn</span>
<span id="annotated-cell-8-12"><a href="#annotated-cell-8-12" aria-hidden="true" tabindex="-1"></a></span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="1">1</button><span id="annotated-cell-8-13" class="code-annotation-target"><a href="#annotated-cell-8-13" aria-hidden="true" tabindex="-1"></a>hnn <span class="op">=</span> HNN(data_dim, hidden_layer_width, n_hidden_layers, n_groups, key<span class="op">=</span>key)</span>
<span id="annotated-cell-8-14"><a href="#annotated-cell-8-14" aria-hidden="true" tabindex="-1"></a>init_fn, apply_fn <span class="op">=</span> get_init_apply_fns(hnn)</span>
<span id="annotated-cell-8-15"><a href="#annotated-cell-8-15" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_fn()</span>
<span id="annotated-cell-8-16"><a href="#annotated-cell-8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-8-17"><a href="#annotated-cell-8-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inference_loop(key, step_fn, initial_state, num_samples):</span>
<span id="annotated-cell-8-18"><a href="#annotated-cell-8-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> one_step(state, key):</span>
<span id="annotated-cell-8-19"><a href="#annotated-cell-8-19" aria-hidden="true" tabindex="-1"></a>        state, _ <span class="op">=</span> step_fn(key, state)</span>
<span id="annotated-cell-8-20"><a href="#annotated-cell-8-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state, state</span>
<span id="annotated-cell-8-21"><a href="#annotated-cell-8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-8-22"><a href="#annotated-cell-8-22" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> jr.split(key, num_samples)</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-8" data-target-annotation="2">2</button><span id="annotated-cell-8-23" class="code-annotation-target"><a href="#annotated-cell-8-23" aria-hidden="true" tabindex="-1"></a>    _, states <span class="op">=</span> jax.lax.scan(one_step, initial_state, keys)</span>
<span id="annotated-cell-8-24"><a href="#annotated-cell-8-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> states</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-8" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="13" data-code-annotation="1">Instantiate the HNN model</span>
</dd>
<dt data-target-cell="annotated-cell-8" data-target-annotation="2">2</dt>
<dd>
<span data-code-cell="annotated-cell-8" data-code-lines="23" data-code-annotation="2">In Jax, we can use the <code>scan</code> method to iterate over a function</span>
</dd>
</dl>
</div>
</div>
<p>Next, we write the (log)-prior function for the parameters of the model, as well as the log-likelihood.</p>
<div id="prior-likelihood" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="annotated-cell-9"><pre class="sourceCode python code-annotation-code code-with-copy code-annotated"><code class="sourceCode python"><span id="annotated-cell-9-1"><a href="#annotated-cell-9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logprior_fn(params):</span>
<span id="annotated-cell-9-2"><a href="#annotated-cell-9-2" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> tfd.Normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="annotated-cell-9-3"><a href="#annotated-cell-9-3" aria-hidden="true" tabindex="-1"></a>    leaves, _ <span class="op">=</span> jax.tree_util.tree_flatten(params)</span>
<span id="annotated-cell-9-4"><a href="#annotated-cell-9-4" aria-hidden="true" tabindex="-1"></a>    flat_params <span class="op">=</span> jnp.concatenate([jnp.ravel(a) <span class="cf">for</span> a <span class="kw">in</span> leaves])</span>
<span id="annotated-cell-9-5"><a href="#annotated-cell-9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(normal.log_prob(flat_params))</span>
<span id="annotated-cell-9-6"><a href="#annotated-cell-9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-7"><a href="#annotated-cell-9-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logprior_fn_of_hnn(params, model):</span>
<span id="annotated-cell-9-8"><a href="#annotated-cell-9-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""p(w) where w is NN(X; w)"""</span></span>
<span id="annotated-cell-9-9"><a href="#annotated-cell-9-9" aria-hidden="true" tabindex="-1"></a>    lp <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="annotated-cell-9-10"><a href="#annotated-cell-9-10" aria-hidden="true" tabindex="-1"></a>    half_normal <span class="op">=</span> tfd.HalfNormal(<span class="fl">1.0</span>)</span>
<span id="annotated-cell-9-11"><a href="#annotated-cell-9-11" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> tfd.Normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="annotated-cell-9-12"><a href="#annotated-cell-9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> params.layers:</span>
<span id="annotated-cell-9-13"><a href="#annotated-cell-9-13" aria-hidden="true" tabindex="-1"></a>        lp <span class="op">+=</span> normal.log_prob(layer.mu).<span class="bu">sum</span>()</span>
<span id="annotated-cell-9-14"><a href="#annotated-cell-9-14" aria-hidden="true" tabindex="-1"></a>        lp <span class="op">+=</span> normal.log_prob(layer.eps).<span class="bu">sum</span>()</span>
<span id="annotated-cell-9-15"><a href="#annotated-cell-9-15" aria-hidden="true" tabindex="-1"></a>        lp <span class="op">+=</span> half_normal.log_prob(layer.std).<span class="bu">sum</span>()</span>
<span id="annotated-cell-9-16"><a href="#annotated-cell-9-16" aria-hidden="true" tabindex="-1"></a>    lp <span class="op">+=</span> logprior_fn(params.out)</span>
<span id="annotated-cell-9-17"><a href="#annotated-cell-9-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lp</span>
<span id="annotated-cell-9-18"><a href="#annotated-cell-9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-19"><a href="#annotated-cell-9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-20"><a href="#annotated-cell-9-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loglikelihood_fn(params, X, Y, mask, fill_value, model):</span>
<span id="annotated-cell-9-21"><a href="#annotated-cell-9-21" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""p(Y|Y_=NN(X; w))"""</span></span>
<span id="annotated-cell-9-22"><a href="#annotated-cell-9-22" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> jnp.ravel(apply_fn(params, X))</span>
<button class="code-annotation-anchor" data-target-cell="annotated-cell-9" data-target-annotation="1">1</button><span id="annotated-cell-9-23" class="code-annotation-target"><a href="#annotated-cell-9-23" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> jnp.where(jnp.ravel(mask[:, :, <span class="dv">0</span>]) <span class="op">==</span> fill_value, <span class="dv">0</span>, logits)</span>
<span id="annotated-cell-9-24"><a href="#annotated-cell-9-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(tfd.Bernoulli(logits).log_prob(jnp.ravel(Y)))</span>
<span id="annotated-cell-9-25"><a href="#annotated-cell-9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-26"><a href="#annotated-cell-9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="annotated-cell-9-27"><a href="#annotated-cell-9-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logdensity_fn_of_hnn(params, X, Y, mask, fill_value, model):</span>
<span id="annotated-cell-9-28"><a href="#annotated-cell-9-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logprior_fn_of_hnn(params, model) <span class="op">+</span> loglikelihood_fn(params, X, Y, mask, fill_value, model)</span><div class="code-annotation-gutter-bg"></div><div class="code-annotation-gutter"></div></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-annotation">
<dl class="code-annotation-container-hidden code-annotation-container-grid">
<dt data-target-cell="annotated-cell-9" data-target-annotation="1">1</dt>
<dd>
<span data-code-cell="annotated-cell-9" data-code-lines="23" data-code-annotation="1">apply the mask: where the mask has the fill value, the logits should also be zero</span>
</dd>
</dl>
</div>
</div>
<p>And some utility functions for extracting the model predictions</p>
<div id="utils" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_predictions(model, samples, X, mask, fill_value, key):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    vectorized_apply <span class="op">=</span> jax.vmap(apply_fn, in_axes<span class="op">=</span>(<span class="dv">0</span>, <span class="va">None</span>), out_axes<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> vectorized_apply(samples, X)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> tfd.Bernoulli(logits<span class="op">=</span>z).sample(seed<span class="op">=</span>key)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    mask_reshaped <span class="op">=</span> jnp.broadcast_to(</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        jnp.mean(mask, axis<span class="op">=-</span><span class="dv">1</span>).reshape(mask.shape[<span class="dv">0</span>], mask.shape[<span class="dv">1</span>], <span class="dv">1</span>), predictions.shape</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> jnp.where(mask_reshaped <span class="op">==</span> fill_value, jnp.nan, predictions)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_mean_predictions(predictions, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean prediction and confidence interval around median</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    mean_prediction <span class="op">=</span> jnp.nanmean(predictions, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_prediction <span class="op">&gt;</span> threshold</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_and_eval(</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    key,</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    initial_position,  <span class="co"># Passed from `init_fn` of init/apply function conversion of Equinox NN</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    logdensity_fn,</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>    Y_train,</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>    mask_train,</span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    fill_value,</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>    X_test,</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>    grid,</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        warmup_key,</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        inference_key,</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        train_key,</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        test_key,</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        grid_key,</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">=</span> jr.split(key, <span class="dv">5</span>)</span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialization</span></span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> partial(logdensity_fn, X<span class="op">=</span>X_train, Y<span class="op">=</span>Y_train, mask<span class="op">=</span>mask_train, fill_value<span class="op">=</span>fill_value, model<span class="op">=</span>model)</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warm up</span></span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    adapt <span class="op">=</span> blackjax.window_adaptation(blackjax.nuts, logprob)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    (final_state, params), _ <span class="op">=</span> adapt.run(warmup_key, initial_position, num_warmup)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>    step_fn <span class="op">=</span> blackjax.nuts(logprob, <span class="op">**</span>params).step</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> inference_loop(inference_key, step_fn, final_state, num_samples)</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> states.position</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation</span></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions(model, samples, X_train, mask_train, fill_value, train_key)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>    Y_pred_train <span class="op">=</span> get_mean_predictions(predictions)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions(model, samples, X_test, mask_test, fill_value, test_key)</span>
<span id="cb8-57"><a href="#cb8-57" aria-hidden="true" tabindex="-1"></a>    Y_pred_test <span class="op">=</span> get_mean_predictions(predictions)</span>
<span id="cb8-58"><a href="#cb8-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-59"><a href="#cb8-59" aria-hidden="true" tabindex="-1"></a>    pred_grid <span class="op">=</span> get_predictions(model, samples, grid, mask_grid, fill_value, grid_key)</span>
<span id="cb8-60"><a href="#cb8-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-61"><a href="#cb8-61" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y_pred_train, Y_pred_test, pred_grid</span>
<span id="cb8-62"><a href="#cb8-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-63"><a href="#cb8-63" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_mask(targets, predictions, mask, fill_value):</span>
<span id="cb8-64"><a href="#cb8-64" aria-hidden="true" tabindex="-1"></a>    targets, predictions, mask <span class="op">=</span> jnp.ravel(targets), jnp.ravel(predictions), jnp.ravel(mask[:,:,<span class="dv">0</span>])</span>
<span id="cb8-65"><a href="#cb8-65" aria-hidden="true" tabindex="-1"></a>    positions_to_omit <span class="op">=</span> jnp.where(mask <span class="op">==</span> fill_value)[<span class="dv">0</span>]</span>
<span id="cb8-66"><a href="#cb8-66" aria-hidden="true" tabindex="-1"></a>    filtered_targets, filtered_predictions <span class="op">=</span> jnp.delete(targets, positions_to_omit), jnp.delete(predictions, positions_to_omit)</span>
<span id="cb8-67"><a href="#cb8-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> filtered_targets,filtered_predictions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We create a 3D-grid (n_groups, 100*100, 2) to get the model’s predictions (with the respective probability) and run the model:</p>
<div id="model-training" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> jnp.mgrid[<span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>, <span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>].reshape((<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T </span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>grid_3d <span class="op">=</span> jnp.repeat(grid[<span class="va">None</span>, ...], n_groups, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>mask_grid <span class="op">=</span> jnp.ones(grid_3d.shape)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>key, inference_key <span class="op">=</span> jr.split(key)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>(Ys_hierarchical_pred_train, Ys_hierarchical_pred_test, ppc_grid) <span class="op">=</span> fit_and_eval(</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    inference_key,</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    params,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    hnn,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    logdensity_fn_of_hnn,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    Xs_train,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    Ys_train,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    mask_train,</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>    fill_value,</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>    Xs_test,</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    grid_3d,</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span>num_warmup,</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span>num_samples,</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="reverse-the-mask" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>filtered_Ys_train, filtered_Ys_hierarchical_pred_train <span class="op">=</span> reverse_mask(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    Ys_train, Ys_hierarchical_pred_train, mask_train, fill_value</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>filtered_Ys_test, filtered_Ys_hierarchical_pred_test <span class="op">=</span> reverse_mask(</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    Ys_test, Ys_hierarchical_pred_test, mask_test, fill_value</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="calculate-train-accuracy" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train accuracy = </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(<span class="dv">100</span> <span class="op">*</span> jnp.mean(filtered_Ys_hierarchical_pred_train <span class="op">==</span> filtered_Ys_train)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Train accuracy = 91.91%</code></pre>
</div>
</div>
<div id="calculate-test-accuracy" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test accuracy = </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(<span class="dv">100</span> <span class="op">*</span> jnp.mean(filtered_Ys_hierarchical_pred_test <span class="op">==</span> filtered_Ys_test)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Test accuracy = 91.89%</code></pre>
</div>
</div>
<div id="cell-plotting-function" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_surfaces_hierarchical(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">12</span>), nrows<span class="op">=</span>nrows, ncols<span class="op">=</span>ncols, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, Y_pred, Y_true, ax) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(Xs_train, Ys_hierarchical_pred_train, Ys_train, axes.flatten())):</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>        ax.contourf(</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">0</span>].reshape((<span class="dv">100</span>, <span class="dv">100</span>)),</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">1</span>].reshape((<span class="dv">100</span>, <span class="dv">100</span>)),</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            ppc_grid[:, i, :].mean(axis<span class="op">=</span><span class="dv">0</span>).reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>cmap,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>            zorder<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>            ax.scatter(X[Y_true <span class="op">==</span> i, <span class="dv">0</span>], X[Y_true <span class="op">==</span> i, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"w"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, s<span class="op">=</span><span class="fl">20.0</span>, zorder<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>            ax.scatter(</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>                X[Y_true <span class="op">==</span> i, <span class="dv">0</span>],</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>                X[Y_true <span class="op">==</span> i, <span class="dv">1</span>],</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span>cmap(<span class="bu">float</span>(i)),</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span><span class="fl">10.0</span>,</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>                zorder<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        ax.set_xticks([])</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        ax.set_yticks([])</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>        ax.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a><span class="co"># %%</span></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>plot_decision_surfaces_hierarchical(nrows<span class="op">=</span>n_grps_sq, ncols<span class="op">=</span>n_grps_sq)</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"figures/hbnn_decision_boundaries.png"</span>, bbox_inches<span class="op">=</span><span class="st">"tight"</span>)</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/plotting-function-output-1.png" id="plotting-function" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="summary" class="level1" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> Summary</h1>
<p>Great! We successfully wrote a model that can work with varying inpout shapes, by using padding and masking. Written in this way, the Bayesian Hierarchical Neural Network is much more generally applicable compared to an implementation assuming groups of equal sizes.</p>
<div id="show-package-versions" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> print_versions <span class="im">import</span> print_versions</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>print_versions(<span class="bu">globals</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>jax==0.6.1
equinox==0.12.2
numpy==2.3.0
matplotlib==3.10.3
blackjax==1.2.5
jaxlib==0.6.1</code></pre>
</div>
</div>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/juliopterix\.github\.io\/jmehringer\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
        for (let i=0; i<annoteTargets.length; i++) {
          const annoteTarget = annoteTargets[i];
          const targetCell = annoteTarget.getAttribute("data-target-cell");
          const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
          const contentFn = () => {
            const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
            if (content) {
              const tipContent = content.cloneNode(true);
              tipContent.classList.add("code-annotation-tip-content");
              return tipContent.outerHTML;
            }
          }
          const config = {
            allowHTML: true,
            content: contentFn,
            onShow: (instance) => {
              selectCodeLines(instance.reference);
              instance.reference.classList.add('code-annotation-active');
              window.tippy.hideAll();
            },
            onHide: (instance) => {
              unselectCodeLines();
              instance.reference.classList.remove('code-annotation-active');
            },
            maxWidth: 300,
            delay: [50, 0],
            duration: [200, 0],
            offset: [5, 10],
            arrow: true,
            appendTo: function(el) {
              return el.parentElement.parentElement.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'quarto',
            placement: 'right',
            popperOptions: {
              modifiers: [
              {
                name: 'flip',
                options: {
                  flipVariations: false, // true by default
                  allowedAutoPlacements: ['right'],
                  fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
                },
              },
              {
                name: 'preventOverflow',
                options: {
                  mainAxis: false,
                  altAxis: false
                }
              }
              ]        
            }      
          };
          window.tippy(annoteTarget, config); 
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb18" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Hierarchical Bayesian Neural Networks with variable group sizes"</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "Standard implementations and publically available tutorials of Hierarchical Bayesian Neural Networks (HBNNs) lack the ability to work with varying numbers of training examples per group. In this entry, I will show you how we can make use of padding and masking in order to be able to train HBNNs."</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> </span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - name: Julius Mehringer</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="co">      url: https://github.com/juliopterix</span></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="co">      orcid: 0009-0006-2827-9156</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2025-06-13"</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [Probabilistic Machine Learning, nonlinear model, Hierarchical Modeling, jax, blackjax] # self-defined categories</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a><span class="an">format:</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co">  html:</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="co">    code-fold: false</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a><span class="co">    code-tools: true</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co">    page-layout: full</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> figures/hbnn_decision_boundaries.png</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false # setting this to `true` will prevent your post from appearing on your listing page until you're ready!</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a><span class="an">self-contained:</span><span class="co"> false</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a><span class="co">  freeze: auto</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="an">code-annotations:</span><span class="co"> hover</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>Bayesian Modeling is a very suitable choice if you want to obtain the uncertainty associated with the predictions of a model. Here, typically a Markov Chain Monte Carlo estimator is used, which explores any stationary distribution and recovers (asymptotically) consistent estimators and thus those samplers are of primary interest, because we can essentially (re-)construct any distribution. This can also be the joint distribution of the weights of a Neural Network! This could be incredibly promising, since we can combine the powers from statistical modeling techniques with the of universal function approximation from Neural Networks. To this end, there are recent <span class="co">[</span><span class="ot">voices</span><span class="co">](https://raw.githubusercontent.com/mlresearch/v235/main/assets/papamarkou24b/papamarkou24b.pdf)</span> arguing why <span class="co">[</span><span class="ot">Bayesian Deep Learning is a promising avenue</span><span class="co">]</span>{.mark}.</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>In Bayesian Modeling, *Hierarchical Bayesian Modeling* is a special kind of model specification, helping the sampler to expore the distribution of interest. It is in fact so powerful that once you know about it, you can't unsee applications of it (primarily in the Sciences). Hierarchical Modeling can be used if you have some **grouped** structure in your dataset, e.g. if products can be assigned to clusters that share some properties. More on this technique in @sec-hbnns.</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>There are some very useful blog entries and notebooks out there (e.g. <span class="co">[</span><span class="ot">by Thomas Wiecki</span><span class="co">](https://twiecki.io/blog/2018/08/13/hierarchical_bayesian_neural_network/)</span> using Theano and PyMC3 and <span class="co">[</span><span class="ot">this repo</span><span class="co">](https://github.com/homerjed/hbnn)</span> using a more recent version of JAX). However, those examples only work with the critical assumption that the group sizes are all **of the same size**. In reality, this is rarely the case, of course.</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>Here, I will show you how you can implement a <span class="in">`Hierarchical Bayesian Neural Network`</span> irrespective of the group sizes you observe in your dataset.</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>The notebook is structured as follows:</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>✍️ Create a dataset: Binary classification with unequal observations per group</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>🧠 What's the modelling approach and why does it work?</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>👾 Code the model</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>🚀 Run the model and evaluate</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="fu"># Setup and dummy data generation</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a>Let's first import the libraries we'll need:</span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-libraries</span></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Tuple</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> date</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> warnings <span class="im">import</span> filterwarnings</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.random <span class="im">as</span> jr</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> scale</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> blackjax</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability.substrates.jax.distributions <span class="im">as</span> tfd</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jr.key(<span class="bu">int</span>(date.today().strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">"</span>)))</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>cmap <span class="op">=</span> mpl.colormaps[<span class="st">"RdYlGn"</span>]</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>Thoughout the notebook, we'll use the standard <span class="in">`two-moons`</span> dataset, being a binary classification problem. @fig-two-moons shows how the dataset looks with some training examples.</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-two-moons</span></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "The standard two-moons dataset plot"</span></span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a>noise <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> make_moons(noise<span class="op">=</span>noise, n_samples<span class="op">=</span><span class="dv">2000</span>)</span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a>    plt.scatter(X[Y <span class="op">==</span> i, <span class="dv">0</span>], X[Y <span class="op">==</span> i, <span class="dv">1</span>], color<span class="op">=</span>cmap(<span class="bu">float</span>(i)), label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>Next, let's choose some values for the data generation of our grouped dataset. We'll create several groups with a random number of samples, choose some settings for our Neural Network implementation and set two parameters for the MCMC-Algorithm: the number of 'warmup' samples (which will be discarded after the model fitting finished) and the number of sampling steps.</span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: data-and-model-settings</span></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a><span class="co"># Data</span></span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>n_groups <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>n_grps_sq <span class="op">=</span> <span class="bu">int</span>(np.sqrt(n_groups))</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a>n_samples <span class="op">=</span> np.random.randint(<span class="dv">10</span>, <span class="dv">200</span>, size<span class="op">=</span>n_groups)</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a><span class="co"># MLP params</span></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>data_dim <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>hidden_layer_width <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>n_hidden_layers <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a><span class="co"># Sampling</span></span>
<span id="cb18-105"><a href="#cb18-105" aria-hidden="true" tabindex="-1"></a>num_warmup <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb18-106"><a href="#cb18-106" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">2000</span></span>
<span id="cb18-107"><a href="#cb18-107" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-108"><a href="#cb18-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-109"><a href="#cb18-109" aria-hidden="true" tabindex="-1"></a>We then write a function which rotates the dataset in the 2-D space a bit and generate the datasets, store them in lists:</span>
<span id="cb18-110"><a href="#cb18-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-113"><a href="#cb18-113" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-114"><a href="#cb18-114" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: generate-groups</span></span>
<span id="cb18-115"><a href="#cb18-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-116"><a href="#cb18-116" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rotate(X, deg):</span>
<span id="cb18-117"><a href="#cb18-117" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.radians(deg)</span>
<span id="cb18-118"><a href="#cb18-118" aria-hidden="true" tabindex="-1"></a>    c, s <span class="op">=</span> np.cos(theta), np.sin(theta)</span>
<span id="cb18-119"><a href="#cb18-119" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.matrix([[c, <span class="op">-</span>s], [s, c]])</span>
<span id="cb18-120"><a href="#cb18-120" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.dot(R)</span>
<span id="cb18-121"><a href="#cb18-121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(X)</span>
<span id="cb18-122"><a href="#cb18-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-123"><a href="#cb18-123" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">31</span>)</span>
<span id="cb18-124"><a href="#cb18-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-125"><a href="#cb18-125" aria-hidden="true" tabindex="-1"></a>Xs, Ys, gs <span class="op">=</span> [], [], []</span>
<span id="cb18-126"><a href="#cb18-126" aria-hidden="true" tabindex="-1"></a>Xs_train, Ys_train, gs_train, Xs_test, Ys_test, gs_test <span class="op">=</span> [], [], [], [], [], []</span>
<span id="cb18-127"><a href="#cb18-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-128"><a href="#cb18-128" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_groups):</span>
<span id="cb18-129"><a href="#cb18-129" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate data with 2 classes that are not linearly separable</span></span>
<span id="cb18-130"><a href="#cb18-130" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> make_moons(noise<span class="op">=</span>noise, n_samples<span class="op">=</span>n_samples[i])</span>
<span id="cb18-131"><a href="#cb18-131" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> scale(X)</span>
<span id="cb18-132"><a href="#cb18-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-133"><a href="#cb18-133" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rotate the points randomly for each category</span></span>
<span id="cb18-134"><a href="#cb18-134" aria-hidden="true" tabindex="-1"></a>    rotate_by <span class="op">=</span> np.random.randn() <span class="op">*</span> <span class="fl">90.0</span></span>
<span id="cb18-135"><a href="#cb18-135" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> rotate(X, rotate_by)</span>
<span id="cb18-136"><a href="#cb18-136" aria-hidden="true" tabindex="-1"></a>    Xs.append(X)</span>
<span id="cb18-137"><a href="#cb18-137" aria-hidden="true" tabindex="-1"></a>    Ys.append(Y)</span>
<span id="cb18-138"><a href="#cb18-138" aria-hidden="true" tabindex="-1"></a>    gs.append(X.shape[<span class="dv">0</span>])</span>
<span id="cb18-139"><a href="#cb18-139" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, Y_train, Y_test <span class="op">=</span> train_test_split(X, Y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">31</span>)</span>
<span id="cb18-140"><a href="#cb18-140" aria-hidden="true" tabindex="-1"></a>    Xs_train.append(X_train)</span>
<span id="cb18-141"><a href="#cb18-141" aria-hidden="true" tabindex="-1"></a>    Ys_train.append(Y_train)</span>
<span id="cb18-142"><a href="#cb18-142" aria-hidden="true" tabindex="-1"></a>    gs_train.append(X_train.shape[<span class="dv">0</span>])</span>
<span id="cb18-143"><a href="#cb18-143" aria-hidden="true" tabindex="-1"></a>    Xs_test.append(X_test)</span>
<span id="cb18-144"><a href="#cb18-144" aria-hidden="true" tabindex="-1"></a>    Ys_test.append(Y_test)</span>
<span id="cb18-145"><a href="#cb18-145" aria-hidden="true" tabindex="-1"></a>    gs_test.append(X_test.shape[<span class="dv">0</span>])</span>
<span id="cb18-146"><a href="#cb18-146" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-147"><a href="#cb18-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-148"><a href="#cb18-148" aria-hidden="true" tabindex="-1"></a>Next, we pad the entries in our list of datasets such that all the entries have the same shape: the shape of the largest dataset. We also create a mask, marking the elements of the entries which were padded. Padding works here, because we can disregard the masked positions in our datasets in the <span class="in">`loglikelihood function`</span>.</span>
<span id="cb18-149"><a href="#cb18-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-152"><a href="#cb18-152" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-153"><a href="#cb18-153" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: apply-padding</span></span>
<span id="cb18-154"><a href="#cb18-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-155"><a href="#cb18-155" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pad_arrays(arrays, fill_value):</span>
<span id="cb18-156"><a href="#cb18-156" aria-hidden="true" tabindex="-1"></a>    max_size <span class="op">=</span> <span class="bu">max</span>(array.shape[<span class="dv">0</span>] <span class="cf">for</span> array <span class="kw">in</span> arrays)</span>
<span id="cb18-157"><a href="#cb18-157" aria-hidden="true" tabindex="-1"></a>    padded_arrays <span class="op">=</span> []</span>
<span id="cb18-158"><a href="#cb18-158" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> array <span class="kw">in</span> arrays:</span>
<span id="cb18-159"><a href="#cb18-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> array.ndim <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb18-160"><a href="#cb18-160" aria-hidden="true" tabindex="-1"></a>            padding <span class="op">=</span> (<span class="dv">0</span>, max_size <span class="op">-</span> array.shape[<span class="dv">0</span>]) </span>
<span id="cb18-161"><a href="#cb18-161" aria-hidden="true" tabindex="-1"></a>            padded_array <span class="op">=</span> jnp.pad(array, padding, mode<span class="op">=</span><span class="st">"constant"</span>, constant_values<span class="op">=</span>fill_value)</span>
<span id="cb18-162"><a href="#cb18-162" aria-hidden="true" tabindex="-1"></a>            padded_arrays.append(padded_array[:, np.newaxis])  </span>
<span id="cb18-163"><a href="#cb18-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb18-164"><a href="#cb18-164" aria-hidden="true" tabindex="-1"></a>            padding <span class="op">=</span> ((<span class="dv">0</span>, max_size <span class="op">-</span> array.shape[<span class="dv">0</span>]), (<span class="dv">0</span>, <span class="dv">0</span>))  </span>
<span id="cb18-165"><a href="#cb18-165" aria-hidden="true" tabindex="-1"></a>            padded_array <span class="op">=</span> jnp.pad(array, padding, mode<span class="op">=</span><span class="st">"constant"</span>, constant_values<span class="op">=</span>fill_value)</span>
<span id="cb18-166"><a href="#cb18-166" aria-hidden="true" tabindex="-1"></a>            padded_arrays.append(padded_array)</span>
<span id="cb18-167"><a href="#cb18-167" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> padded_arrays</span>
<span id="cb18-168"><a href="#cb18-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-169"><a href="#cb18-169" aria-hidden="true" tabindex="-1"></a><span class="co"># Stack group arrays and create a mask</span></span>
<span id="cb18-170"><a href="#cb18-170" aria-hidden="true" tabindex="-1"></a>fill_value <span class="op">=</span> <span class="fl">1e5</span></span>
<span id="cb18-171"><a href="#cb18-171" aria-hidden="true" tabindex="-1"></a>Xs_train <span class="op">=</span> jnp.stack(pad_arrays(Xs_train, fill_value))</span>
<span id="cb18-172"><a href="#cb18-172" aria-hidden="true" tabindex="-1"></a>Ys_train <span class="op">=</span> jnp.stack(pad_arrays(Ys_train, fill_value)).squeeze(axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-173"><a href="#cb18-173" aria-hidden="true" tabindex="-1"></a>Xs_test <span class="op">=</span> jnp.stack(pad_arrays(Xs_test, fill_value))</span>
<span id="cb18-174"><a href="#cb18-174" aria-hidden="true" tabindex="-1"></a>Ys_test <span class="op">=</span> jnp.stack(pad_arrays(Ys_test, fill_value)).squeeze(axis<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb18-175"><a href="#cb18-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-176"><a href="#cb18-176" aria-hidden="true" tabindex="-1"></a>mask_train <span class="op">=</span> jnp.where(Xs_train <span class="op">==</span> fill_value, fill_value, <span class="dv">1</span>)</span>
<span id="cb18-177"><a href="#cb18-177" aria-hidden="true" tabindex="-1"></a>mask_test <span class="op">=</span> jnp.where(Xs_test <span class="op">==</span> fill_value, fill_value, <span class="dv">1</span>)</span>
<span id="cb18-178"><a href="#cb18-178" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-179"><a href="#cb18-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-180"><a href="#cb18-180" aria-hidden="true" tabindex="-1"></a>In @fig-show-datasets you can see how the datasets look and how many entries the individual groups got:</span>
<span id="cb18-181"><a href="#cb18-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-184"><a href="#cb18-184" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-185"><a href="#cb18-185" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-show-datasets</span></span>
<span id="cb18-186"><a href="#cb18-186" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Groups with varying numbers of training examples"</span></span>
<span id="cb18-187"><a href="#cb18-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-188"><a href="#cb18-188" aria-hidden="true" tabindex="-1"></a><span class="co"># utility function for plotting</span></span>
<span id="cb18-189"><a href="#cb18-189" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> closest_factors(n):</span>
<span id="cb18-190"><a href="#cb18-190" aria-hidden="true" tabindex="-1"></a>    a, b <span class="op">=</span> <span class="dv">1</span>, n</span>
<span id="cb18-191"><a href="#cb18-191" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, <span class="bu">int</span>(n<span class="op">**</span><span class="fl">0.5</span>) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb18-192"><a href="#cb18-192" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n <span class="op">%</span> i <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-193"><a href="#cb18-193" aria-hidden="true" tabindex="-1"></a>            a, b <span class="op">=</span> i, n <span class="op">//</span> i </span>
<span id="cb18-194"><a href="#cb18-194" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> a, b</span>
<span id="cb18-195"><a href="#cb18-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-196"><a href="#cb18-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-197"><a href="#cb18-197" aria-hidden="true" tabindex="-1"></a><span class="co"># Number of rows and columns for subplots</span></span>
<span id="cb18-198"><a href="#cb18-198" aria-hidden="true" tabindex="-1"></a>n_cols, n_rows <span class="op">=</span> closest_factors(n_groups)</span>
<span id="cb18-199"><a href="#cb18-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-200"><a href="#cb18-200" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(n_rows, n_cols, figsize<span class="op">=</span>(n_cols<span class="op">*</span><span class="dv">3</span>, n_rows<span class="op">*</span><span class="dv">2</span>), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-201"><a href="#cb18-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-202"><a href="#cb18-202" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten axes array for easy iteration</span></span>
<span id="cb18-203"><a href="#cb18-203" aria-hidden="true" tabindex="-1"></a>axes <span class="op">=</span> axes.flatten()</span>
<span id="cb18-204"><a href="#cb18-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-205"><a href="#cb18-205" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, (X, Y, group_size, ax) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(Xs_train, Ys_train, gs_train, axes)):</span>
<span id="cb18-206"><a href="#cb18-206" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> c <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb18-207"><a href="#cb18-207" aria-hidden="true" tabindex="-1"></a>        ax.scatter(X[Y <span class="op">==</span> c, <span class="dv">0</span>], X[Y <span class="op">==</span> c, <span class="dv">1</span>], color<span class="op">=</span>cmap(<span class="bu">float</span>(c)), label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>c<span class="sc">}</span><span class="ss">"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb18-208"><a href="#cb18-208" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([])</span>
<span id="cb18-209"><a href="#cb18-209" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([])</span>
<span id="cb18-210"><a href="#cb18-210" aria-hidden="true" tabindex="-1"></a>    ax.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-211"><a href="#cb18-211" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(title<span class="op">=</span><span class="ss">f"Category </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, N_training = </span><span class="sc">{</span>group_size<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb18-212"><a href="#cb18-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-213"><a href="#cb18-213" aria-hidden="true" tabindex="-1"></a><span class="co"># Hide any unused subplots</span></span>
<span id="cb18-214"><a href="#cb18-214" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_groups, <span class="bu">len</span>(axes)):</span>
<span id="cb18-215"><a href="#cb18-215" aria-hidden="true" tabindex="-1"></a>    fig.delaxes(axes[j])</span>
<span id="cb18-216"><a href="#cb18-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-217"><a href="#cb18-217" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb18-218"><a href="#cb18-218" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-219"><a href="#cb18-219" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-220"><a href="#cb18-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-221"><a href="#cb18-221" aria-hidden="true" tabindex="-1"></a><span class="fu"># A Bayesian Hierarchical Neural Network {#sec-hbnns}</span></span>
<span id="cb18-222"><a href="#cb18-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-223"><a href="#cb18-223" aria-hidden="true" tabindex="-1"></a>Now, let's dig into why modeling this dataset with a <span class="in">`hierarchical`</span> Neural Network might make sense. Beware: we'll use some vocabulary from statistical modeling.</span>
<span id="cb18-224"><a href="#cb18-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-225"><a href="#cb18-225" aria-hidden="true" tabindex="-1"></a>The prominent probabilistic modeler <span class="co">[</span><span class="ot">Michael Betancourt</span><span class="co">](https://betanalpha.github.io)</span> provides an <span class="co">[</span><span class="ot">in depth introduction to the foundations of hierarchical modeling</span><span class="co">](https://betanalpha.github.io/assets/case_studies/hierarchical_modeling.html)</span>. Conceptually, hierarchical modeling is an approach if there is a latent population that we can couple context-dependent parameters to. In our dataset, we assume there is some homogeneous structure *withtin* the groups, whereas the groups may be different *between* them. This means that we can actually share the weights of the group's individual Neural Networks **across** all networks, because the task (binary classification with some Z-shape) is similar; even though the individual groups are all oriented differently in the 2-D space (difference between the groups).</span>
<span id="cb18-226"><a href="#cb18-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-227"><a href="#cb18-227" aria-hidden="true" tabindex="-1"></a>::: callout-tip</span>
<span id="cb18-228"><a href="#cb18-228" aria-hidden="true" tabindex="-1"></a><span class="fu">## When does a Hierarchical Bayesian Neural Network make the most sense?</span></span>
<span id="cb18-229"><a href="#cb18-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-230"><a href="#cb18-230" aria-hidden="true" tabindex="-1"></a>Summarizing, a HBNN requires that:</span>
<span id="cb18-231"><a href="#cb18-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-232"><a href="#cb18-232" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>you can make use of a grouping structure in your dataset</span>
<span id="cb18-233"><a href="#cb18-233" aria-hidden="true" tabindex="-1"></a><span class="ss">-   </span>the data generating process of the individual groups is similar</span>
<span id="cb18-234"><a href="#cb18-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-235"><a href="#cb18-235" aria-hidden="true" tabindex="-1"></a>It is the strongest, if you have a small-ish number of observations (probably in relation to the difficulty of the learning task?), since in this case 'traditional' approaches will fail (see @tip-book-example).</span>
<span id="cb18-236"><a href="#cb18-236" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-237"><a href="#cb18-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-238"><a href="#cb18-238" aria-hidden="true" tabindex="-1"></a>::: {#tip-book-example .callout-tip collapse="true"}</span>
<span id="cb18-239"><a href="#cb18-239" aria-hidden="true" tabindex="-1"></a><span class="fu">## Recap: Running a separate model per group (example from the Blackjax Sampling book)</span></span>
<span id="cb18-240"><a href="#cb18-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-241"><a href="#cb18-241" aria-hidden="true" tabindex="-1"></a>Since it's covered in the <span class="co">[</span><span class="ot">HBNN-tutorial from the BlackJax Sampling Book</span><span class="co">](https://blackjax-devs.github.io/sampling-book/models/hierarchical_bnn.html)</span>, I will just copy-paste the code here for reference and the curious (and because I don't want to loose this example if the Sampling Book disappears at some point).</span>
<span id="cb18-242"><a href="#cb18-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-243"><a href="#cb18-243" aria-hidden="true" tabindex="-1"></a>**The key takeaway is this**: If you fit the models separately, there will be not enough training examples for the Neural Network to capture the nonlinear relationship separating the two classes.</span>
<span id="cb18-244"><a href="#cb18-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-247"><a href="#cb18-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-248"><a href="#cb18-248" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: recap-sampling-book-example</span></span>
<span id="cb18-249"><a href="#cb18-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-250"><a href="#cb18-250" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb18-251"><a href="#cb18-251" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb18-252"><a href="#cb18-252" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datetime <span class="im">import</span> date</span>
<span id="cb18-253"><a href="#cb18-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-254"><a href="#cb18-254" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb18-255"><a href="#cb18-255" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> warnings <span class="im">import</span> filterwarnings</span>
<span id="cb18-256"><a href="#cb18-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-257"><a href="#cb18-257" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax <span class="im">import</span> linen <span class="im">as</span> nn</span>
<span id="cb18-258"><a href="#cb18-258" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> flax.linen.initializers <span class="im">import</span> ones</span>
<span id="cb18-259"><a href="#cb18-259" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb18-260"><a href="#cb18-260" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb18-261"><a href="#cb18-261" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow_probability.substrates.jax.distributions <span class="im">as</span> tfd</span>
<span id="cb18-262"><a href="#cb18-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-263"><a href="#cb18-263" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_moons</span>
<span id="cb18-264"><a href="#cb18-264" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> scale</span>
<span id="cb18-265"><a href="#cb18-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-266"><a href="#cb18-266" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> blackjax</span>
<span id="cb18-267"><a href="#cb18-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-268"><a href="#cb18-268" aria-hidden="true" tabindex="-1"></a>filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb18-269"><a href="#cb18-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-270"><a href="#cb18-270" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib <span class="im">as</span> mpl</span>
<span id="cb18-271"><a href="#cb18-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-272"><a href="#cb18-272" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"axes.spines.right"</span>] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb18-273"><a href="#cb18-273" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"axes.spines.top"</span>] <span class="op">=</span> <span class="va">False</span></span>
<span id="cb18-274"><a href="#cb18-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-275"><a href="#cb18-275" aria-hidden="true" tabindex="-1"></a>rng_key <span class="op">=</span> jax.random.key(<span class="bu">int</span>(date.today().strftime(<span class="st">"%Y%m</span><span class="sc">%d</span><span class="st">"</span>)))</span>
<span id="cb18-276"><a href="#cb18-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-277"><a href="#cb18-277" aria-hidden="true" tabindex="-1"></a>n_groups_tut <span class="op">=</span> <span class="dv">18</span></span>
<span id="cb18-278"><a href="#cb18-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-279"><a href="#cb18-279" aria-hidden="true" tabindex="-1"></a>n_grps_sq_tut <span class="op">=</span> <span class="bu">int</span>(np.sqrt(n_groups_tut))</span>
<span id="cb18-280"><a href="#cb18-280" aria-hidden="true" tabindex="-1"></a>n_samples_tut <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb18-281"><a href="#cb18-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-282"><a href="#cb18-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-283"><a href="#cb18-283" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> rotate_tut(X, deg):</span>
<span id="cb18-284"><a href="#cb18-284" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> np.radians(deg)</span>
<span id="cb18-285"><a href="#cb18-285" aria-hidden="true" tabindex="-1"></a>    c, s <span class="op">=</span> np.cos(theta), np.sin(theta)</span>
<span id="cb18-286"><a href="#cb18-286" aria-hidden="true" tabindex="-1"></a>    R <span class="op">=</span> np.matrix([[c, <span class="op">-</span>s], [s, c]])</span>
<span id="cb18-287"><a href="#cb18-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-288"><a href="#cb18-288" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> X.dot(R)</span>
<span id="cb18-289"><a href="#cb18-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-290"><a href="#cb18-290" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.asarray(X)</span>
<span id="cb18-291"><a href="#cb18-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-292"><a href="#cb18-292" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">31</span>)</span>
<span id="cb18-293"><a href="#cb18-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-294"><a href="#cb18-294" aria-hidden="true" tabindex="-1"></a>Xs_tut, Ys_tut <span class="op">=</span> [], []</span>
<span id="cb18-295"><a href="#cb18-295" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_groups_tut):</span>
<span id="cb18-296"><a href="#cb18-296" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Generate data with 2 classes that are not linearly separable</span></span>
<span id="cb18-297"><a href="#cb18-297" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> make_moons(noise<span class="op">=</span><span class="fl">0.3</span>, n_samples<span class="op">=</span>n_samples_tut)</span>
<span id="cb18-298"><a href="#cb18-298" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> scale(X)</span>
<span id="cb18-299"><a href="#cb18-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-300"><a href="#cb18-300" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Rotate the points randomly for each category</span></span>
<span id="cb18-301"><a href="#cb18-301" aria-hidden="true" tabindex="-1"></a>    rotate_by <span class="op">=</span> np.random.randn() <span class="op">*</span> <span class="fl">90.0</span></span>
<span id="cb18-302"><a href="#cb18-302" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> rotate_tut(X, rotate_by)</span>
<span id="cb18-303"><a href="#cb18-303" aria-hidden="true" tabindex="-1"></a>    Xs_tut.append(X)</span>
<span id="cb18-304"><a href="#cb18-304" aria-hidden="true" tabindex="-1"></a>    Ys_tut.append(Y)</span>
<span id="cb18-305"><a href="#cb18-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-306"><a href="#cb18-306" aria-hidden="true" tabindex="-1"></a>Xs_tut <span class="op">=</span> jnp.stack(Xs_tut)</span>
<span id="cb18-307"><a href="#cb18-307" aria-hidden="true" tabindex="-1"></a>Ys_tut <span class="op">=</span> jnp.stack(Ys_tut)</span>
<span id="cb18-308"><a href="#cb18-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-309"><a href="#cb18-309" aria-hidden="true" tabindex="-1"></a>Xs_tut_train <span class="op">=</span> Xs_tut[:, : n_samples_tut <span class="op">//</span> <span class="dv">2</span>, :]</span>
<span id="cb18-310"><a href="#cb18-310" aria-hidden="true" tabindex="-1"></a>Xs_tut_test <span class="op">=</span> Xs_tut[:, n_samples_tut <span class="op">//</span> <span class="dv">2</span> :, :]</span>
<span id="cb18-311"><a href="#cb18-311" aria-hidden="true" tabindex="-1"></a>Ys_tut_train <span class="op">=</span> Ys_tut[:, : n_samples_tut <span class="op">//</span> <span class="dv">2</span>]</span>
<span id="cb18-312"><a href="#cb18-312" aria-hidden="true" tabindex="-1"></a>Ys_tut_test <span class="op">=</span> Ys_tut[:, n_samples_tut <span class="op">//</span> <span class="dv">2</span> :]</span>
<span id="cb18-313"><a href="#cb18-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-314"><a href="#cb18-314" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> jnp.mgrid[<span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>, <span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>].reshape((<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T</span>
<span id="cb18-315"><a href="#cb18-315" aria-hidden="true" tabindex="-1"></a>grid_3d <span class="op">=</span> jnp.repeat(grid[<span class="va">None</span>, ...], n_groups_tut, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-316"><a href="#cb18-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-317"><a href="#cb18-317" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inference_loop_tut(rng_key, step_fn, initial_state, num_samples):</span>
<span id="cb18-318"><a href="#cb18-318" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> one_step(state, rng_key):</span>
<span id="cb18-319"><a href="#cb18-319" aria-hidden="true" tabindex="-1"></a>        state, _ <span class="op">=</span> step_fn(rng_key, state)</span>
<span id="cb18-320"><a href="#cb18-320" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state, state</span>
<span id="cb18-321"><a href="#cb18-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-322"><a href="#cb18-322" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> jax.random.split(rng_key, num_samples)</span>
<span id="cb18-323"><a href="#cb18-323" aria-hidden="true" tabindex="-1"></a>    _, states <span class="op">=</span> jax.lax.scan(one_step, initial_state, keys)</span>
<span id="cb18-324"><a href="#cb18-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-325"><a href="#cb18-325" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> states</span>
<span id="cb18-326"><a href="#cb18-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-327"><a href="#cb18-327" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_predictions_tut(model, samples, X, rng_key):</span>
<span id="cb18-328"><a href="#cb18-328" aria-hidden="true" tabindex="-1"></a>    vectorized_apply <span class="op">=</span> jax.vmap(model.<span class="bu">apply</span>, in_axes<span class="op">=</span>(<span class="dv">0</span>, <span class="va">None</span>), out_axes<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-329"><a href="#cb18-329" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> vectorized_apply(samples, X)</span>
<span id="cb18-330"><a href="#cb18-330" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> tfd.Bernoulli(logits<span class="op">=</span>z).sample(seed<span class="op">=</span>rng_key)</span>
<span id="cb18-331"><a href="#cb18-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-332"><a href="#cb18-332" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-333"><a href="#cb18-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-334"><a href="#cb18-334" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_mean_predictions_tut(predictions, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb18-335"><a href="#cb18-335" aria-hidden="true" tabindex="-1"></a>    <span class="co"># compute mean prediction and confidence interval around median</span></span>
<span id="cb18-336"><a href="#cb18-336" aria-hidden="true" tabindex="-1"></a>    mean_prediction <span class="op">=</span> jnp.mean(predictions, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-337"><a href="#cb18-337" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_prediction <span class="op">&gt;</span> threshold</span>
<span id="cb18-338"><a href="#cb18-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-339"><a href="#cb18-339" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_and_eval_tut(</span>
<span id="cb18-340"><a href="#cb18-340" aria-hidden="true" tabindex="-1"></a>    rng_key,</span>
<span id="cb18-341"><a href="#cb18-341" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb18-342"><a href="#cb18-342" aria-hidden="true" tabindex="-1"></a>    logdensity_fn,</span>
<span id="cb18-343"><a href="#cb18-343" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb18-344"><a href="#cb18-344" aria-hidden="true" tabindex="-1"></a>    Y_train,</span>
<span id="cb18-345"><a href="#cb18-345" aria-hidden="true" tabindex="-1"></a>    X_test,</span>
<span id="cb18-346"><a href="#cb18-346" aria-hidden="true" tabindex="-1"></a>    grid,</span>
<span id="cb18-347"><a href="#cb18-347" aria-hidden="true" tabindex="-1"></a>    n_groups<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb18-348"><a href="#cb18-348" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb18-349"><a href="#cb18-349" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span><span class="dv">2000</span>,</span>
<span id="cb18-350"><a href="#cb18-350" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb18-351"><a href="#cb18-351" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb18-352"><a href="#cb18-352" aria-hidden="true" tabindex="-1"></a>        init_key,</span>
<span id="cb18-353"><a href="#cb18-353" aria-hidden="true" tabindex="-1"></a>        warmup_key,</span>
<span id="cb18-354"><a href="#cb18-354" aria-hidden="true" tabindex="-1"></a>        inference_key,</span>
<span id="cb18-355"><a href="#cb18-355" aria-hidden="true" tabindex="-1"></a>        train_key,</span>
<span id="cb18-356"><a href="#cb18-356" aria-hidden="true" tabindex="-1"></a>        test_key,</span>
<span id="cb18-357"><a href="#cb18-357" aria-hidden="true" tabindex="-1"></a>        grid_key,</span>
<span id="cb18-358"><a href="#cb18-358" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">=</span> jax.random.split(rng_key, <span class="dv">6</span>)</span>
<span id="cb18-359"><a href="#cb18-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-360"><a href="#cb18-360" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_groups <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb18-361"><a href="#cb18-361" aria-hidden="true" tabindex="-1"></a>        initial_position <span class="op">=</span> model.init(init_key, jnp.ones(X_train.shape[<span class="op">-</span><span class="dv">1</span>]))</span>
<span id="cb18-362"><a href="#cb18-362" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb18-363"><a href="#cb18-363" aria-hidden="true" tabindex="-1"></a>        initial_position <span class="op">=</span> model.init(init_key, jnp.ones(X_train.shape))</span>
<span id="cb18-364"><a href="#cb18-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-365"><a href="#cb18-365" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialization</span></span>
<span id="cb18-366"><a href="#cb18-366" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> partial(logdensity_fn, X<span class="op">=</span>X_train, Y<span class="op">=</span>Y_train, model<span class="op">=</span>model)</span>
<span id="cb18-367"><a href="#cb18-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-368"><a href="#cb18-368" aria-hidden="true" tabindex="-1"></a>    <span class="co"># warm up</span></span>
<span id="cb18-369"><a href="#cb18-369" aria-hidden="true" tabindex="-1"></a>    adapt <span class="op">=</span> blackjax.window_adaptation(blackjax.nuts, logprob)</span>
<span id="cb18-370"><a href="#cb18-370" aria-hidden="true" tabindex="-1"></a>    (final_state, params), _ <span class="op">=</span> adapt.run(warmup_key, initial_position, num_warmup)</span>
<span id="cb18-371"><a href="#cb18-371" aria-hidden="true" tabindex="-1"></a>    step_fn <span class="op">=</span> blackjax.nuts(logprob, <span class="op">**</span>params).step</span>
<span id="cb18-372"><a href="#cb18-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-373"><a href="#cb18-373" aria-hidden="true" tabindex="-1"></a>    <span class="co"># inference</span></span>
<span id="cb18-374"><a href="#cb18-374" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> inference_loop_tut(inference_key, step_fn, final_state, num_samples)</span>
<span id="cb18-375"><a href="#cb18-375" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> states.position</span>
<span id="cb18-376"><a href="#cb18-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-377"><a href="#cb18-377" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluation</span></span>
<span id="cb18-378"><a href="#cb18-378" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions_tut(model, samples, X_train, train_key)</span>
<span id="cb18-379"><a href="#cb18-379" aria-hidden="true" tabindex="-1"></a>    Y_pred_train <span class="op">=</span> get_mean_predictions_tut(predictions)</span>
<span id="cb18-380"><a href="#cb18-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-381"><a href="#cb18-381" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions_tut(model, samples, X_test, test_key)</span>
<span id="cb18-382"><a href="#cb18-382" aria-hidden="true" tabindex="-1"></a>    Y_pred_test <span class="op">=</span> get_mean_predictions_tut(predictions)</span>
<span id="cb18-383"><a href="#cb18-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-384"><a href="#cb18-384" aria-hidden="true" tabindex="-1"></a>    pred_grid <span class="op">=</span> get_predictions_tut(model, samples, grid, grid_key)</span>
<span id="cb18-385"><a href="#cb18-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-386"><a href="#cb18-386" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y_pred_train, Y_pred_test, pred_grid</span>
<span id="cb18-387"><a href="#cb18-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-388"><a href="#cb18-388" aria-hidden="true" tabindex="-1"></a><span class="co"># MLP params</span></span>
<span id="cb18-389"><a href="#cb18-389" aria-hidden="true" tabindex="-1"></a>hidden_layer_width_tut <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb18-390"><a href="#cb18-390" aria-hidden="true" tabindex="-1"></a>n_hidden_layers_tut <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb18-391"><a href="#cb18-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-392"><a href="#cb18-392" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NN(nn.Module):</span>
<span id="cb18-393"><a href="#cb18-393" aria-hidden="true" tabindex="-1"></a>    n_hidden_layers: <span class="bu">int</span></span>
<span id="cb18-394"><a href="#cb18-394" aria-hidden="true" tabindex="-1"></a>    layer_width: <span class="bu">int</span></span>
<span id="cb18-395"><a href="#cb18-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-396"><a href="#cb18-396" aria-hidden="true" tabindex="-1"></a>    <span class="at">@nn.compact</span></span>
<span id="cb18-397"><a href="#cb18-397" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb18-398"><a href="#cb18-398" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_hidden_layers):</span>
<span id="cb18-399"><a href="#cb18-399" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> nn.Dense(features<span class="op">=</span><span class="va">self</span>.layer_width)(x)</span>
<span id="cb18-400"><a href="#cb18-400" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> nn.tanh(x)</span>
<span id="cb18-401"><a href="#cb18-401" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> nn.Dense(features<span class="op">=</span><span class="dv">1</span>)(x)</span>
<span id="cb18-402"><a href="#cb18-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-403"><a href="#cb18-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-404"><a href="#cb18-404" aria-hidden="true" tabindex="-1"></a>bnn <span class="op">=</span> NN(n_hidden_layers_tut, hidden_layer_width_tut)</span>
<span id="cb18-405"><a href="#cb18-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-406"><a href="#cb18-406" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logprior_fn_tut(params):</span>
<span id="cb18-407"><a href="#cb18-407" aria-hidden="true" tabindex="-1"></a>    leaves, _ <span class="op">=</span> jax.tree_util.tree_flatten(params)</span>
<span id="cb18-408"><a href="#cb18-408" aria-hidden="true" tabindex="-1"></a>    flat_params <span class="op">=</span> jnp.concatenate([jnp.ravel(a) <span class="cf">for</span> a <span class="kw">in</span> leaves])</span>
<span id="cb18-409"><a href="#cb18-409" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(tfd.Normal(<span class="dv">0</span>, <span class="dv">1</span>).log_prob(flat_params))</span>
<span id="cb18-410"><a href="#cb18-410" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-411"><a href="#cb18-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-412"><a href="#cb18-412" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loglikelihood_fn_tut(params, X, Y, model):</span>
<span id="cb18-413"><a href="#cb18-413" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> jnp.ravel(model.<span class="bu">apply</span>(params, X))</span>
<span id="cb18-414"><a href="#cb18-414" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(tfd.Bernoulli(logits).log_prob(Y))</span>
<span id="cb18-415"><a href="#cb18-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-416"><a href="#cb18-416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-417"><a href="#cb18-417" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logdensity_fn_of_bnn_tut(params, X, Y, model):</span>
<span id="cb18-418"><a href="#cb18-418" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logprior_fn_tut(params) <span class="op">+</span> loglikelihood_fn_tut(params, X, Y, model)</span>
<span id="cb18-419"><a href="#cb18-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-420"><a href="#cb18-420" aria-hidden="true" tabindex="-1"></a>rng_key, eval_key <span class="op">=</span> jax.random.split(rng_key)</span>
<span id="cb18-421"><a href="#cb18-421" aria-hidden="true" tabindex="-1"></a>keys <span class="op">=</span> jax.random.split(eval_key, n_groups_tut)</span>
<span id="cb18-422"><a href="#cb18-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-423"><a href="#cb18-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-424"><a href="#cb18-424" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_and_eval_single_mlp_tut(key, X_train, Y_train, X_test):</span>
<span id="cb18-425"><a href="#cb18-425" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fit_and_eval_tut(</span>
<span id="cb18-426"><a href="#cb18-426" aria-hidden="true" tabindex="-1"></a>        key, bnn, logdensity_fn_of_bnn_tut, X_train, Y_train, X_test, grid, n_groups<span class="op">=</span><span class="va">None</span></span>
<span id="cb18-427"><a href="#cb18-427" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-428"><a href="#cb18-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-429"><a href="#cb18-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-430"><a href="#cb18-430" aria-hidden="true" tabindex="-1"></a>Ys_pred_train, Ys_pred_test, ppc_grid_single <span class="op">=</span> jax.vmap(fit_and_eval_single_mlp_tut)(</span>
<span id="cb18-431"><a href="#cb18-431" aria-hidden="true" tabindex="-1"></a>    keys, Xs_tut_train, Ys_tut_train, Xs_tut_test</span>
<span id="cb18-432"><a href="#cb18-432" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-433"><a href="#cb18-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-434"><a href="#cb18-434" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_surfaces_non_hierarchical_tut(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb18-435"><a href="#cb18-435" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(</span>
<span id="cb18-436"><a href="#cb18-436" aria-hidden="true" tabindex="-1"></a>        figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">12</span>), nrows<span class="op">=</span>nrows, ncols<span class="op">=</span>ncols, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span></span>
<span id="cb18-437"><a href="#cb18-437" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-438"><a href="#cb18-438" aria-hidden="true" tabindex="-1"></a>    axes <span class="op">=</span> axes.flatten()</span>
<span id="cb18-439"><a href="#cb18-439" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, Y_pred, Y_true, ax) <span class="kw">in</span> <span class="bu">enumerate</span>(</span>
<span id="cb18-440"><a href="#cb18-440" aria-hidden="true" tabindex="-1"></a>        <span class="bu">zip</span>(Xs_tut_train, Ys_pred_train, Ys_tut_train, axes)</span>
<span id="cb18-441"><a href="#cb18-441" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb18-442"><a href="#cb18-442" aria-hidden="true" tabindex="-1"></a>        ax.contourf(</span>
<span id="cb18-443"><a href="#cb18-443" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">0</span>].reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb18-444"><a href="#cb18-444" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">1</span>].reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb18-445"><a href="#cb18-445" aria-hidden="true" tabindex="-1"></a>            ppc_grid_single[i, ...].mean(axis<span class="op">=</span><span class="dv">0</span>).reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb18-446"><a href="#cb18-446" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>cmap,</span>
<span id="cb18-447"><a href="#cb18-447" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-448"><a href="#cb18-448" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb18-449"><a href="#cb18-449" aria-hidden="true" tabindex="-1"></a>            ax.scatter(</span>
<span id="cb18-450"><a href="#cb18-450" aria-hidden="true" tabindex="-1"></a>                X[Y_true <span class="op">==</span> i, <span class="dv">0</span>], X[Y_true <span class="op">==</span> i, <span class="dv">1</span>], </span>
<span id="cb18-451"><a href="#cb18-451" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span>cmap(<span class="bu">float</span>(i)), label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, alpha<span class="op">=</span><span class="fl">.8</span>)</span>
<span id="cb18-452"><a href="#cb18-452" aria-hidden="true" tabindex="-1"></a>        ax.legend()</span>
<span id="cb18-453"><a href="#cb18-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-454"><a href="#cb18-454" aria-hidden="true" tabindex="-1"></a>plot_decision_surfaces_non_hierarchical_tut(nrows<span class="op">=</span>n_grps_sq_tut, ncols<span class="op">=</span>n_grps_sq_tut)</span>
<span id="cb18-455"><a href="#cb18-455" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-456"><a href="#cb18-456" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-457"><a href="#cb18-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-458"><a href="#cb18-458" aria-hidden="true" tabindex="-1"></a><span class="fu"># Coding the model</span></span>
<span id="cb18-459"><a href="#cb18-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-460"><a href="#cb18-460" aria-hidden="true" tabindex="-1"></a>Now we're ready for the modeling code. To get an overview of the model we we're about to create, have a look at @fig-model-depict. There, we have **groups** $g=1:G$ with the respective weight matrices $w^g_{l}$ for the input, hidden and output layers $l$.</span>
<span id="cb18-461"><a href="#cb18-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-462"><a href="#cb18-462" aria-hidden="true" tabindex="-1"></a>The group weights are drawn from a Normal distribution, which, in a *non-centered* form means</span>
<span id="cb18-463"><a href="#cb18-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-464"><a href="#cb18-464" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb18-465"><a href="#cb18-465" aria-hidden="true" tabindex="-1"></a>w^g_l = \mu_{l} + \epsilon^g_{l} \sigma_l</span>
<span id="cb18-466"><a href="#cb18-466" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb18-467"><a href="#cb18-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-468"><a href="#cb18-468" aria-hidden="true" tabindex="-1"></a>with</span>
<span id="cb18-469"><a href="#cb18-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-470"><a href="#cb18-470" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb18-471"><a href="#cb18-471" aria-hidden="true" tabindex="-1"></a>\begin{align}\mu_l &amp;\sim \mathcal{N}(0,1) <span class="sc">\\</span>\epsilon^g_l &amp;\sim \mathcal{N}(0,1) <span class="sc">\\</span>\sigma_l &amp;= 1\end{align}</span>
<span id="cb18-472"><a href="#cb18-472" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb18-473"><a href="#cb18-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-474"><a href="#cb18-474" aria-hidden="true" tabindex="-1"></a>This non-centered formulation simplifies the space to be explored by our sampler.</span>
<span id="cb18-475"><a href="#cb18-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-476"><a href="#cb18-476" aria-hidden="true" tabindex="-1"></a>::: callout-note</span>
<span id="cb18-477"><a href="#cb18-477" aria-hidden="true" tabindex="-1"></a><span class="fu">## Centered Formulation</span></span>
<span id="cb18-478"><a href="#cb18-478" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-479"><a href="#cb18-479" aria-hidden="true" tabindex="-1"></a>We don't model the individual weights directly. This 'Centered Formulation' would mean $w^g_l \sim \mathcal{N}(\mu_l, \sigma_l)$ with usually a $\mathcal{N}$ prior for $\mu$; and a $\mathcal{N}^+$ prior for $\sigma$.</span>
<span id="cb18-480"><a href="#cb18-480" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb18-481"><a href="#cb18-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-482"><a href="#cb18-482" aria-hidden="true" tabindex="-1"></a>!<span class="co">[</span><span class="ot">Graphical Depiction of the model we want to train (slightly adapted from [the HBNN sampling book reference](https://blackjax-devs.github.io/sampling-book/models/hierarchical_bnn.html))</span><span class="co">](figures/model_composition.svg)</span>{#fig-model-depict width="70%"}</span>
<span id="cb18-483"><a href="#cb18-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-484"><a href="#cb18-484" aria-hidden="true" tabindex="-1"></a>In the following code block, we write the model using <span class="co">[</span><span class="ot">Equinox</span><span class="co">](https://docs.kidger.site/equinox/all-of-equinox/)</span>, which in turn uses <span class="co">[</span><span class="ot">JAX</span><span class="co">](https://github.com/jax-ml/jax)</span> for all the numerical routines (e.g. autodiff).</span>
<span id="cb18-485"><a href="#cb18-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-488"><a href="#cb18-488" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-489"><a href="#cb18-489" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: code-HNN</span></span>
<span id="cb18-490"><a href="#cb18-490" aria-hidden="true" tabindex="-1"></a><span class="co">#| code-fold: false</span></span>
<span id="cb18-491"><a href="#cb18-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-492"><a href="#cb18-492" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NonCentredLinear(eqx.Module):                                 <span class="co"># &lt;1&gt;</span></span>
<span id="cb18-493"><a href="#cb18-493" aria-hidden="true" tabindex="-1"></a>    mu: jax.Array</span>
<span id="cb18-494"><a href="#cb18-494" aria-hidden="true" tabindex="-1"></a>    eps: jax.Array</span>
<span id="cb18-495"><a href="#cb18-495" aria-hidden="true" tabindex="-1"></a>    std: jax.Array</span>
<span id="cb18-496"><a href="#cb18-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-497"><a href="#cb18-497" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, in_size, out_size, n_groups, <span class="op">*</span>, key):</span>
<span id="cb18-498"><a href="#cb18-498" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mu <span class="op">=</span> jr.normal(key, (in_size, out_size))               <span class="co"># &lt;2&gt;</span></span>
<span id="cb18-499"><a href="#cb18-499" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> jr.normal(key, (n_groups, in_size, out_size))    <span class="co"># &lt;3&gt;</span></span>
<span id="cb18-500"><a href="#cb18-500" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.std <span class="op">=</span> jnp.ones((<span class="dv">1</span>,))                                   <span class="co"># &lt;4&gt;</span></span>
<span id="cb18-501"><a href="#cb18-501" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-502"><a href="#cb18-502" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):                                          <span class="co"># &lt;5&gt;</span></span>
<span id="cb18-503"><a href="#cb18-503" aria-hidden="true" tabindex="-1"></a>        w <span class="op">=</span> <span class="va">self</span>.mu <span class="op">+</span> <span class="va">self</span>.eps <span class="op">*</span> <span class="va">self</span>.std                           <span class="co"># &lt;5&gt;</span></span>
<span id="cb18-504"><a href="#cb18-504" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x <span class="op">@</span> w                                                <span class="co"># &lt;5&gt;</span></span>
<span id="cb18-505"><a href="#cb18-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-506"><a href="#cb18-506" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-507"><a href="#cb18-507" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> HNN(eqx.Module):                                              <span class="co"># &lt;6&gt;</span></span>
<span id="cb18-508"><a href="#cb18-508" aria-hidden="true" tabindex="-1"></a>    layers: Tuple[NonCentredLinear]</span>
<span id="cb18-509"><a href="#cb18-509" aria-hidden="true" tabindex="-1"></a>    out: eqx.nn.Linear</span>
<span id="cb18-510"><a href="#cb18-510" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-511"><a href="#cb18-511" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, data_dim, layer_width, n_layers, n_groups, <span class="op">*</span>, key): <span class="co"># &lt;7&gt;</span></span>
<span id="cb18-512"><a href="#cb18-512" aria-hidden="true" tabindex="-1"></a>        dims <span class="op">=</span> [data_dim] <span class="op">+</span> [layer_width] <span class="op">*</span> n_layers</span>
<span id="cb18-513"><a href="#cb18-513" aria-hidden="true" tabindex="-1"></a>        layers <span class="op">=</span> []</span>
<span id="cb18-514"><a href="#cb18-514" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> n, (_in, _out) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(dims[:<span class="op">-</span><span class="dv">1</span>], dims[<span class="dv">1</span>:])):</span>
<span id="cb18-515"><a href="#cb18-515" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> NonCentredLinear(_in, _out, n_groups, key<span class="op">=</span>jr.fold_in(key, n)) <span class="co"># &lt;8&gt;</span></span>
<span id="cb18-516"><a href="#cb18-516" aria-hidden="true" tabindex="-1"></a>            layers <span class="op">+=</span> [layer]</span>
<span id="cb18-517"><a href="#cb18-517" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.layers <span class="op">=</span> <span class="bu">tuple</span>(layers)</span>
<span id="cb18-518"><a href="#cb18-518" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.out <span class="op">=</span> eqx.nn.Linear(layer_width, <span class="dv">1</span>, key<span class="op">=</span>key)  <span class="co"># &lt;9&gt;</span></span>
<span id="cb18-519"><a href="#cb18-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-520"><a href="#cb18-520" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb18-521"><a href="#cb18-521" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.layers:</span>
<span id="cb18-522"><a href="#cb18-522" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> layer(x)</span>
<span id="cb18-523"><a href="#cb18-523" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> jax.nn.tanh(x)  <span class="co"># &lt;10&gt;</span></span>
<span id="cb18-524"><a href="#cb18-524" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Vmap over groups and samples</span></span>
<span id="cb18-525"><a href="#cb18-525" aria-hidden="true" tabindex="-1"></a>        o <span class="op">=</span> jax.vmap(jax.vmap(<span class="va">self</span>.out))(x)</span>
<span id="cb18-526"><a href="#cb18-526" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> o</span>
<span id="cb18-527"><a href="#cb18-527" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-528"><a href="#cb18-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-529"><a href="#cb18-529" aria-hidden="true" tabindex="-1"></a><span class="ss">1.  </span>Write the Non-centered layers as Equinox module</span>
<span id="cb18-530"><a href="#cb18-530" aria-hidden="true" tabindex="-1"></a><span class="ss">2.  </span>Initialize the weights in $\mu$ as standard Normal, one for each layer</span>
<span id="cb18-531"><a href="#cb18-531" aria-hidden="true" tabindex="-1"></a><span class="ss">3.  </span>Initialize the weights in $\epsilon$ as standard Normal, one for each layer **and** group</span>
<span id="cb18-532"><a href="#cb18-532" aria-hidden="true" tabindex="-1"></a><span class="ss">4.  </span>Initialize $\sigma$ as 1.</span>
<span id="cb18-533"><a href="#cb18-533" aria-hidden="true" tabindex="-1"></a><span class="ss">5.  </span>Non-centered combination of the matrices and the dot-product of $x$ and $w$.</span>
<span id="cb18-534"><a href="#cb18-534" aria-hidden="true" tabindex="-1"></a><span class="ss">6.  </span>Write the Hierarchical Neural Network as Equinox module</span>
<span id="cb18-535"><a href="#cb18-535" aria-hidden="true" tabindex="-1"></a><span class="ss">7.  </span>In this implementation, all layers have the same width</span>
<span id="cb18-536"><a href="#cb18-536" aria-hidden="true" tabindex="-1"></a><span class="ss">8.  </span>Create all hidden layers</span>
<span id="cb18-537"><a href="#cb18-537" aria-hidden="true" tabindex="-1"></a><span class="ss">9.  </span>Final linear layer</span>
<span id="cb18-538"><a href="#cb18-538" aria-hidden="true" tabindex="-1"></a><span class="ss">10. </span>Choose $\tanh$ as activation function</span>
<span id="cb18-539"><a href="#cb18-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-540"><a href="#cb18-540" aria-hidden="true" tabindex="-1"></a>Next, we instantiate the HNN model and write some code that Equinox needs.</span>
<span id="cb18-541"><a href="#cb18-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-544"><a href="#cb18-544" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-545"><a href="#cb18-545" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: instantiate-model</span></span>
<span id="cb18-546"><a href="#cb18-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-547"><a href="#cb18-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-548"><a href="#cb18-548" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_init_apply_fns(model):</span>
<span id="cb18-549"><a href="#cb18-549" aria-hidden="true" tabindex="-1"></a>    params, static <span class="op">=</span> eqx.partition(model, eqx.is_inexact_array)</span>
<span id="cb18-550"><a href="#cb18-550" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-551"><a href="#cb18-551" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> init_fn():</span>
<span id="cb18-552"><a href="#cb18-552" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> params</span>
<span id="cb18-553"><a href="#cb18-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-554"><a href="#cb18-554" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> apply_fn(_params, x):</span>
<span id="cb18-555"><a href="#cb18-555" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> eqx.combine(_params, static)</span>
<span id="cb18-556"><a href="#cb18-556" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model(x)</span>
<span id="cb18-557"><a href="#cb18-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-558"><a href="#cb18-558" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> init_fn, apply_fn</span>
<span id="cb18-559"><a href="#cb18-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-560"><a href="#cb18-560" aria-hidden="true" tabindex="-1"></a>hnn <span class="op">=</span> HNN(data_dim, hidden_layer_width, n_hidden_layers, n_groups, key<span class="op">=</span>key) <span class="co"># &lt;1&gt;</span></span>
<span id="cb18-561"><a href="#cb18-561" aria-hidden="true" tabindex="-1"></a>init_fn, apply_fn <span class="op">=</span> get_init_apply_fns(hnn)</span>
<span id="cb18-562"><a href="#cb18-562" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> init_fn()</span>
<span id="cb18-563"><a href="#cb18-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-564"><a href="#cb18-564" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> inference_loop(key, step_fn, initial_state, num_samples):</span>
<span id="cb18-565"><a href="#cb18-565" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> one_step(state, key):</span>
<span id="cb18-566"><a href="#cb18-566" aria-hidden="true" tabindex="-1"></a>        state, _ <span class="op">=</span> step_fn(key, state)</span>
<span id="cb18-567"><a href="#cb18-567" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> state, state</span>
<span id="cb18-568"><a href="#cb18-568" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-569"><a href="#cb18-569" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> jr.split(key, num_samples)</span>
<span id="cb18-570"><a href="#cb18-570" aria-hidden="true" tabindex="-1"></a>    _, states <span class="op">=</span> jax.lax.scan(one_step, initial_state, keys) <span class="co"># &lt;2&gt;</span></span>
<span id="cb18-571"><a href="#cb18-571" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> states</span>
<span id="cb18-572"><a href="#cb18-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-573"><a href="#cb18-573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-574"><a href="#cb18-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-575"><a href="#cb18-575" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Instantiate the HNN model</span>
<span id="cb18-576"><a href="#cb18-576" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>In Jax, we can use the <span class="in">`scan`</span> method to iterate over a function</span>
<span id="cb18-577"><a href="#cb18-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-578"><a href="#cb18-578" aria-hidden="true" tabindex="-1"></a>Next, we write the (log)-prior function for the parameters of the model, as well as the log-likelihood.</span>
<span id="cb18-581"><a href="#cb18-581" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-582"><a href="#cb18-582" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: prior-likelihood</span></span>
<span id="cb18-583"><a href="#cb18-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-584"><a href="#cb18-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-585"><a href="#cb18-585" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logprior_fn(params):</span>
<span id="cb18-586"><a href="#cb18-586" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> tfd.Normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb18-587"><a href="#cb18-587" aria-hidden="true" tabindex="-1"></a>    leaves, _ <span class="op">=</span> jax.tree_util.tree_flatten(params)</span>
<span id="cb18-588"><a href="#cb18-588" aria-hidden="true" tabindex="-1"></a>    flat_params <span class="op">=</span> jnp.concatenate([jnp.ravel(a) <span class="cf">for</span> a <span class="kw">in</span> leaves])</span>
<span id="cb18-589"><a href="#cb18-589" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(normal.log_prob(flat_params))</span>
<span id="cb18-590"><a href="#cb18-590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-591"><a href="#cb18-591" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logprior_fn_of_hnn(params, model):</span>
<span id="cb18-592"><a href="#cb18-592" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""p(w) where w is NN(X; w)"""</span></span>
<span id="cb18-593"><a href="#cb18-593" aria-hidden="true" tabindex="-1"></a>    lp <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-594"><a href="#cb18-594" aria-hidden="true" tabindex="-1"></a>    half_normal <span class="op">=</span> tfd.HalfNormal(<span class="fl">1.0</span>)</span>
<span id="cb18-595"><a href="#cb18-595" aria-hidden="true" tabindex="-1"></a>    normal <span class="op">=</span> tfd.Normal(<span class="fl">0.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb18-596"><a href="#cb18-596" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> layer <span class="kw">in</span> params.layers:</span>
<span id="cb18-597"><a href="#cb18-597" aria-hidden="true" tabindex="-1"></a>        lp <span class="op">+=</span> normal.log_prob(layer.mu).<span class="bu">sum</span>()</span>
<span id="cb18-598"><a href="#cb18-598" aria-hidden="true" tabindex="-1"></a>        lp <span class="op">+=</span> normal.log_prob(layer.eps).<span class="bu">sum</span>()</span>
<span id="cb18-599"><a href="#cb18-599" aria-hidden="true" tabindex="-1"></a>        lp <span class="op">+=</span> half_normal.log_prob(layer.std).<span class="bu">sum</span>()</span>
<span id="cb18-600"><a href="#cb18-600" aria-hidden="true" tabindex="-1"></a>    lp <span class="op">+=</span> logprior_fn(params.out)</span>
<span id="cb18-601"><a href="#cb18-601" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lp</span>
<span id="cb18-602"><a href="#cb18-602" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-603"><a href="#cb18-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-604"><a href="#cb18-604" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loglikelihood_fn(params, X, Y, mask, fill_value, model):</span>
<span id="cb18-605"><a href="#cb18-605" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""p(Y|Y_=NN(X; w))"""</span></span>
<span id="cb18-606"><a href="#cb18-606" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> jnp.ravel(apply_fn(params, X))</span>
<span id="cb18-607"><a href="#cb18-607" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> jnp.where(jnp.ravel(mask[:, :, <span class="dv">0</span>]) <span class="op">==</span> fill_value, <span class="dv">0</span>, logits) <span class="co"># &lt;1&gt;</span></span>
<span id="cb18-608"><a href="#cb18-608" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(tfd.Bernoulli(logits).log_prob(jnp.ravel(Y)))</span>
<span id="cb18-609"><a href="#cb18-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-610"><a href="#cb18-610" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-611"><a href="#cb18-611" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logdensity_fn_of_hnn(params, X, Y, mask, fill_value, model):</span>
<span id="cb18-612"><a href="#cb18-612" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> logprior_fn_of_hnn(params, model) <span class="op">+</span> loglikelihood_fn(params, X, Y, mask, fill_value, model)</span>
<span id="cb18-613"><a href="#cb18-613" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-614"><a href="#cb18-614" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>apply the mask: where the mask has the fill value, the logits should also be zero</span>
<span id="cb18-615"><a href="#cb18-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-616"><a href="#cb18-616" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-617"><a href="#cb18-617" aria-hidden="true" tabindex="-1"></a>And some utility functions for extracting the model predictions</span>
<span id="cb18-620"><a href="#cb18-620" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-621"><a href="#cb18-621" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: utils</span></span>
<span id="cb18-622"><a href="#cb18-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-623"><a href="#cb18-623" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-624"><a href="#cb18-624" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_predictions(model, samples, X, mask, fill_value, key):</span>
<span id="cb18-625"><a href="#cb18-625" aria-hidden="true" tabindex="-1"></a>    vectorized_apply <span class="op">=</span> jax.vmap(apply_fn, in_axes<span class="op">=</span>(<span class="dv">0</span>, <span class="va">None</span>), out_axes<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-626"><a href="#cb18-626" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> vectorized_apply(samples, X)</span>
<span id="cb18-627"><a href="#cb18-627" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> tfd.Bernoulli(logits<span class="op">=</span>z).sample(seed<span class="op">=</span>key)</span>
<span id="cb18-628"><a href="#cb18-628" aria-hidden="true" tabindex="-1"></a>    mask_reshaped <span class="op">=</span> jnp.broadcast_to(</span>
<span id="cb18-629"><a href="#cb18-629" aria-hidden="true" tabindex="-1"></a>        jnp.mean(mask, axis<span class="op">=-</span><span class="dv">1</span>).reshape(mask.shape[<span class="dv">0</span>], mask.shape[<span class="dv">1</span>], <span class="dv">1</span>), predictions.shape</span>
<span id="cb18-630"><a href="#cb18-630" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-631"><a href="#cb18-631" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> jnp.where(mask_reshaped <span class="op">==</span> fill_value, jnp.nan, predictions)</span>
<span id="cb18-632"><a href="#cb18-632" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> predictions.squeeze(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb18-633"><a href="#cb18-633" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-634"><a href="#cb18-634" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_mean_predictions(predictions, threshold<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb18-635"><a href="#cb18-635" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean prediction and confidence interval around median</span></span>
<span id="cb18-636"><a href="#cb18-636" aria-hidden="true" tabindex="-1"></a>    mean_prediction <span class="op">=</span> jnp.nanmean(predictions, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-637"><a href="#cb18-637" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mean_prediction <span class="op">&gt;</span> threshold</span>
<span id="cb18-638"><a href="#cb18-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-639"><a href="#cb18-639" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-640"><a href="#cb18-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-641"><a href="#cb18-641" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit_and_eval(</span>
<span id="cb18-642"><a href="#cb18-642" aria-hidden="true" tabindex="-1"></a>    key,</span>
<span id="cb18-643"><a href="#cb18-643" aria-hidden="true" tabindex="-1"></a>    initial_position,  <span class="co"># Passed from `init_fn` of init/apply function conversion of Equinox NN</span></span>
<span id="cb18-644"><a href="#cb18-644" aria-hidden="true" tabindex="-1"></a>    model,</span>
<span id="cb18-645"><a href="#cb18-645" aria-hidden="true" tabindex="-1"></a>    logdensity_fn,</span>
<span id="cb18-646"><a href="#cb18-646" aria-hidden="true" tabindex="-1"></a>    X_train,</span>
<span id="cb18-647"><a href="#cb18-647" aria-hidden="true" tabindex="-1"></a>    Y_train,</span>
<span id="cb18-648"><a href="#cb18-648" aria-hidden="true" tabindex="-1"></a>    mask_train,</span>
<span id="cb18-649"><a href="#cb18-649" aria-hidden="true" tabindex="-1"></a>    fill_value,</span>
<span id="cb18-650"><a href="#cb18-650" aria-hidden="true" tabindex="-1"></a>    X_test,</span>
<span id="cb18-651"><a href="#cb18-651" aria-hidden="true" tabindex="-1"></a>    grid,</span>
<span id="cb18-652"><a href="#cb18-652" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span><span class="dv">20</span>,</span>
<span id="cb18-653"><a href="#cb18-653" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb18-654"><a href="#cb18-654" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb18-655"><a href="#cb18-655" aria-hidden="true" tabindex="-1"></a>    (</span>
<span id="cb18-656"><a href="#cb18-656" aria-hidden="true" tabindex="-1"></a>        warmup_key,</span>
<span id="cb18-657"><a href="#cb18-657" aria-hidden="true" tabindex="-1"></a>        inference_key,</span>
<span id="cb18-658"><a href="#cb18-658" aria-hidden="true" tabindex="-1"></a>        train_key,</span>
<span id="cb18-659"><a href="#cb18-659" aria-hidden="true" tabindex="-1"></a>        test_key,</span>
<span id="cb18-660"><a href="#cb18-660" aria-hidden="true" tabindex="-1"></a>        grid_key,</span>
<span id="cb18-661"><a href="#cb18-661" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">=</span> jr.split(key, <span class="dv">5</span>)</span>
<span id="cb18-662"><a href="#cb18-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-663"><a href="#cb18-663" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialization</span></span>
<span id="cb18-664"><a href="#cb18-664" aria-hidden="true" tabindex="-1"></a>    logprob <span class="op">=</span> partial(logdensity_fn, X<span class="op">=</span>X_train, Y<span class="op">=</span>Y_train, mask<span class="op">=</span>mask_train, fill_value<span class="op">=</span>fill_value, model<span class="op">=</span>model)</span>
<span id="cb18-665"><a href="#cb18-665" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-666"><a href="#cb18-666" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Warm up</span></span>
<span id="cb18-667"><a href="#cb18-667" aria-hidden="true" tabindex="-1"></a>    adapt <span class="op">=</span> blackjax.window_adaptation(blackjax.nuts, logprob)</span>
<span id="cb18-668"><a href="#cb18-668" aria-hidden="true" tabindex="-1"></a>    (final_state, params), _ <span class="op">=</span> adapt.run(warmup_key, initial_position, num_warmup)</span>
<span id="cb18-669"><a href="#cb18-669" aria-hidden="true" tabindex="-1"></a>    step_fn <span class="op">=</span> blackjax.nuts(logprob, <span class="op">**</span>params).step</span>
<span id="cb18-670"><a href="#cb18-670" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-671"><a href="#cb18-671" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Inference</span></span>
<span id="cb18-672"><a href="#cb18-672" aria-hidden="true" tabindex="-1"></a>    states <span class="op">=</span> inference_loop(inference_key, step_fn, final_state, num_samples)</span>
<span id="cb18-673"><a href="#cb18-673" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> states.position</span>
<span id="cb18-674"><a href="#cb18-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-675"><a href="#cb18-675" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluation</span></span>
<span id="cb18-676"><a href="#cb18-676" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions(model, samples, X_train, mask_train, fill_value, train_key)</span>
<span id="cb18-677"><a href="#cb18-677" aria-hidden="true" tabindex="-1"></a>    Y_pred_train <span class="op">=</span> get_mean_predictions(predictions)</span>
<span id="cb18-678"><a href="#cb18-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-679"><a href="#cb18-679" aria-hidden="true" tabindex="-1"></a>    predictions <span class="op">=</span> get_predictions(model, samples, X_test, mask_test, fill_value, test_key)</span>
<span id="cb18-680"><a href="#cb18-680" aria-hidden="true" tabindex="-1"></a>    Y_pred_test <span class="op">=</span> get_mean_predictions(predictions)</span>
<span id="cb18-681"><a href="#cb18-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-682"><a href="#cb18-682" aria-hidden="true" tabindex="-1"></a>    pred_grid <span class="op">=</span> get_predictions(model, samples, grid, mask_grid, fill_value, grid_key)</span>
<span id="cb18-683"><a href="#cb18-683" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-684"><a href="#cb18-684" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y_pred_train, Y_pred_test, pred_grid</span>
<span id="cb18-685"><a href="#cb18-685" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-686"><a href="#cb18-686" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> reverse_mask(targets, predictions, mask, fill_value):</span>
<span id="cb18-687"><a href="#cb18-687" aria-hidden="true" tabindex="-1"></a>    targets, predictions, mask <span class="op">=</span> jnp.ravel(targets), jnp.ravel(predictions), jnp.ravel(mask[:,:,<span class="dv">0</span>])</span>
<span id="cb18-688"><a href="#cb18-688" aria-hidden="true" tabindex="-1"></a>    positions_to_omit <span class="op">=</span> jnp.where(mask <span class="op">==</span> fill_value)[<span class="dv">0</span>]</span>
<span id="cb18-689"><a href="#cb18-689" aria-hidden="true" tabindex="-1"></a>    filtered_targets, filtered_predictions <span class="op">=</span> jnp.delete(targets, positions_to_omit), jnp.delete(predictions, positions_to_omit)</span>
<span id="cb18-690"><a href="#cb18-690" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> filtered_targets,filtered_predictions</span>
<span id="cb18-691"><a href="#cb18-691" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-692"><a href="#cb18-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-693"><a href="#cb18-693" aria-hidden="true" tabindex="-1"></a>We create a 3D-grid (n_groups, 100*100, 2) to get the model's predictions (with the respective probability) and run the model:</span>
<span id="cb18-694"><a href="#cb18-694" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-697"><a href="#cb18-697" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-698"><a href="#cb18-698" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: model-training</span></span>
<span id="cb18-699"><a href="#cb18-699" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-700"><a href="#cb18-700" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-701"><a href="#cb18-701" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> jnp.mgrid[<span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>, <span class="op">-</span><span class="dv">3</span>:<span class="dv">3</span>:<span class="ot">100j</span>].reshape((<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)).T </span>
<span id="cb18-702"><a href="#cb18-702" aria-hidden="true" tabindex="-1"></a>grid_3d <span class="op">=</span> jnp.repeat(grid[<span class="va">None</span>, ...], n_groups, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-703"><a href="#cb18-703" aria-hidden="true" tabindex="-1"></a>mask_grid <span class="op">=</span> jnp.ones(grid_3d.shape)</span>
<span id="cb18-704"><a href="#cb18-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-705"><a href="#cb18-705" aria-hidden="true" tabindex="-1"></a>key, inference_key <span class="op">=</span> jr.split(key)</span>
<span id="cb18-706"><a href="#cb18-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-707"><a href="#cb18-707" aria-hidden="true" tabindex="-1"></a>(Ys_hierarchical_pred_train, Ys_hierarchical_pred_test, ppc_grid) <span class="op">=</span> fit_and_eval(</span>
<span id="cb18-708"><a href="#cb18-708" aria-hidden="true" tabindex="-1"></a>    inference_key,</span>
<span id="cb18-709"><a href="#cb18-709" aria-hidden="true" tabindex="-1"></a>    params,</span>
<span id="cb18-710"><a href="#cb18-710" aria-hidden="true" tabindex="-1"></a>    hnn,</span>
<span id="cb18-711"><a href="#cb18-711" aria-hidden="true" tabindex="-1"></a>    logdensity_fn_of_hnn,</span>
<span id="cb18-712"><a href="#cb18-712" aria-hidden="true" tabindex="-1"></a>    Xs_train,</span>
<span id="cb18-713"><a href="#cb18-713" aria-hidden="true" tabindex="-1"></a>    Ys_train,</span>
<span id="cb18-714"><a href="#cb18-714" aria-hidden="true" tabindex="-1"></a>    mask_train,</span>
<span id="cb18-715"><a href="#cb18-715" aria-hidden="true" tabindex="-1"></a>    fill_value,</span>
<span id="cb18-716"><a href="#cb18-716" aria-hidden="true" tabindex="-1"></a>    Xs_test,</span>
<span id="cb18-717"><a href="#cb18-717" aria-hidden="true" tabindex="-1"></a>    grid_3d,</span>
<span id="cb18-718"><a href="#cb18-718" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span>num_warmup,</span>
<span id="cb18-719"><a href="#cb18-719" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span>num_samples,</span>
<span id="cb18-720"><a href="#cb18-720" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-721"><a href="#cb18-721" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-722"><a href="#cb18-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-723"><a href="#cb18-723" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-726"><a href="#cb18-726" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-727"><a href="#cb18-727" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: reverse-the-mask</span></span>
<span id="cb18-728"><a href="#cb18-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-729"><a href="#cb18-729" aria-hidden="true" tabindex="-1"></a>filtered_Ys_train, filtered_Ys_hierarchical_pred_train <span class="op">=</span> reverse_mask(</span>
<span id="cb18-730"><a href="#cb18-730" aria-hidden="true" tabindex="-1"></a>    Ys_train, Ys_hierarchical_pred_train, mask_train, fill_value</span>
<span id="cb18-731"><a href="#cb18-731" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-732"><a href="#cb18-732" aria-hidden="true" tabindex="-1"></a>filtered_Ys_test, filtered_Ys_hierarchical_pred_test <span class="op">=</span> reverse_mask(</span>
<span id="cb18-733"><a href="#cb18-733" aria-hidden="true" tabindex="-1"></a>    Ys_test, Ys_hierarchical_pred_test, mask_test, fill_value</span>
<span id="cb18-734"><a href="#cb18-734" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-735"><a href="#cb18-735" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-736"><a href="#cb18-736" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-739"><a href="#cb18-739" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-740"><a href="#cb18-740" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: calculate-train-accuracy</span></span>
<span id="cb18-741"><a href="#cb18-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-742"><a href="#cb18-742" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-743"><a href="#cb18-743" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Train accuracy = </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(<span class="dv">100</span> <span class="op">*</span> jnp.mean(filtered_Ys_hierarchical_pred_train <span class="op">==</span> filtered_Ys_train)))</span>
<span id="cb18-744"><a href="#cb18-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-745"><a href="#cb18-745" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-746"><a href="#cb18-746" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-749"><a href="#cb18-749" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-750"><a href="#cb18-750" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: calculate-test-accuracy</span></span>
<span id="cb18-751"><a href="#cb18-751" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-752"><a href="#cb18-752" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Test accuracy = </span><span class="sc">{:.2f}</span><span class="st">%"</span>.<span class="bu">format</span>(<span class="dv">100</span> <span class="op">*</span> jnp.mean(filtered_Ys_hierarchical_pred_test <span class="op">==</span> filtered_Ys_test)))</span>
<span id="cb18-753"><a href="#cb18-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-754"><a href="#cb18-754" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-755"><a href="#cb18-755" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-758"><a href="#cb18-758" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-759"><a href="#cb18-759" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: plotting-function</span></span>
<span id="cb18-760"><a href="#cb18-760" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-761"><a href="#cb18-761" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_decision_surfaces_hierarchical(nrows<span class="op">=</span><span class="dv">2</span>, ncols<span class="op">=</span><span class="dv">2</span>):</span>
<span id="cb18-762"><a href="#cb18-762" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">12</span>), nrows<span class="op">=</span>nrows, ncols<span class="op">=</span>ncols, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-763"><a href="#cb18-763" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-764"><a href="#cb18-764" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (X, Y_pred, Y_true, ax) <span class="kw">in</span> <span class="bu">enumerate</span>(<span class="bu">zip</span>(Xs_train, Ys_hierarchical_pred_train, Ys_train, axes.flatten())):</span>
<span id="cb18-765"><a href="#cb18-765" aria-hidden="true" tabindex="-1"></a>        ax.contourf(</span>
<span id="cb18-766"><a href="#cb18-766" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">0</span>].reshape((<span class="dv">100</span>, <span class="dv">100</span>)),</span>
<span id="cb18-767"><a href="#cb18-767" aria-hidden="true" tabindex="-1"></a>            grid[:, <span class="dv">1</span>].reshape((<span class="dv">100</span>, <span class="dv">100</span>)),</span>
<span id="cb18-768"><a href="#cb18-768" aria-hidden="true" tabindex="-1"></a>            ppc_grid[:, i, :].mean(axis<span class="op">=</span><span class="dv">0</span>).reshape(<span class="dv">100</span>, <span class="dv">100</span>),</span>
<span id="cb18-769"><a href="#cb18-769" aria-hidden="true" tabindex="-1"></a>            cmap<span class="op">=</span>cmap,</span>
<span id="cb18-770"><a href="#cb18-770" aria-hidden="true" tabindex="-1"></a>            zorder<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb18-771"><a href="#cb18-771" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-772"><a href="#cb18-772" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>):</span>
<span id="cb18-773"><a href="#cb18-773" aria-hidden="true" tabindex="-1"></a>            ax.scatter(X[Y_true <span class="op">==</span> i, <span class="dv">0</span>], X[Y_true <span class="op">==</span> i, <span class="dv">1</span>], color<span class="op">=</span><span class="st">"w"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>, s<span class="op">=</span><span class="fl">20.0</span>, zorder<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-774"><a href="#cb18-774" aria-hidden="true" tabindex="-1"></a>            ax.scatter(</span>
<span id="cb18-775"><a href="#cb18-775" aria-hidden="true" tabindex="-1"></a>                X[Y_true <span class="op">==</span> i, <span class="dv">0</span>],</span>
<span id="cb18-776"><a href="#cb18-776" aria-hidden="true" tabindex="-1"></a>                X[Y_true <span class="op">==</span> i, <span class="dv">1</span>],</span>
<span id="cb18-777"><a href="#cb18-777" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span>cmap(<span class="bu">float</span>(i)),</span>
<span id="cb18-778"><a href="#cb18-778" aria-hidden="true" tabindex="-1"></a>                label<span class="op">=</span><span class="ss">f"Class </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>,</span>
<span id="cb18-779"><a href="#cb18-779" aria-hidden="true" tabindex="-1"></a>                alpha<span class="op">=</span><span class="fl">0.8</span>,</span>
<span id="cb18-780"><a href="#cb18-780" aria-hidden="true" tabindex="-1"></a>                s<span class="op">=</span><span class="fl">10.0</span>,</span>
<span id="cb18-781"><a href="#cb18-781" aria-hidden="true" tabindex="-1"></a>                zorder<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb18-782"><a href="#cb18-782" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb18-783"><a href="#cb18-783" aria-hidden="true" tabindex="-1"></a>        ax.set_xticks([])</span>
<span id="cb18-784"><a href="#cb18-784" aria-hidden="true" tabindex="-1"></a>        ax.set_yticks([])</span>
<span id="cb18-785"><a href="#cb18-785" aria-hidden="true" tabindex="-1"></a>        ax.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-786"><a href="#cb18-786" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-787"><a href="#cb18-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-788"><a href="#cb18-788" aria-hidden="true" tabindex="-1"></a><span class="co"># %%</span></span>
<span id="cb18-789"><a href="#cb18-789" aria-hidden="true" tabindex="-1"></a>plot_decision_surfaces_hierarchical(nrows<span class="op">=</span>n_grps_sq, ncols<span class="op">=</span>n_grps_sq)</span>
<span id="cb18-790"><a href="#cb18-790" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">"figures/hbnn_decision_boundaries.png"</span>, bbox_inches<span class="op">=</span><span class="st">"tight"</span>)</span>
<span id="cb18-791"><a href="#cb18-791" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb18-792"><a href="#cb18-792" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb18-793"><a href="#cb18-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-794"><a href="#cb18-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-795"><a href="#cb18-795" aria-hidden="true" tabindex="-1"></a><span class="fu"># Summary</span></span>
<span id="cb18-796"><a href="#cb18-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-797"><a href="#cb18-797" aria-hidden="true" tabindex="-1"></a>Great! We successfully wrote a model that can work with varying inpout shapes, by using padding and masking. Written in this way, the Bayesian Hierarchical Neural Network is much more generally applicable compared to an implementation assuming groups of equal sizes.</span>
<span id="cb18-798"><a href="#cb18-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-799"><a href="#cb18-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-800"><a href="#cb18-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-803"><a href="#cb18-803" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb18-804"><a href="#cb18-804" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: show-package-versions</span></span>
<span id="cb18-805"><a href="#cb18-805" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> print_versions <span class="im">import</span> print_versions</span>
<span id="cb18-806"><a href="#cb18-806" aria-hidden="true" tabindex="-1"></a>print_versions(<span class="bu">globals</span>())</span>
<span id="cb18-807"><a href="#cb18-807" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>