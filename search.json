[
  {
    "objectID": "posts/2025-06-11-welcome/index.html",
    "href": "posts/2025-06-11-welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hi! Glad yourâ€™re here! ðŸ™‚\nItâ€™s time to step into the blog-o-sphere. Iâ€™ve learnt so much from other peoplesâ€™ blogs and would find great joy of someone else would learn something from my learnings as well.\nFor now, this is it on here. I will add some content from time to time.\nCheers!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hierarchical Bayesian Neural Networks with variable group sizes\n\n\n\n\n\n\nprobabilistic machine learning\n\n\nbayesian neural networks\n\n\nhierarchical modeling\n\n\njax\n\n\nblackjax\n\n\n\n\n\n\n\n\n\nJun 11, 2025\n\n\nJulius Mehringer\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nJulius Mehringer\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website contains some personal notes and learnings about mathematical and probabilistic modeling, as well as some special cases of machine learning or artificial intelligence.\nThe avatar was created on this free avatar generator."
  },
  {
    "objectID": "posts/2025-06-11-hierarchical-NN/index.html",
    "href": "posts/2025-06-11-hierarchical-NN/index.html",
    "title": "Hierarchical Bayesian Neural Networks with variable group sizes",
    "section": "",
    "text": "Warning\n\n\n\nThis entry is seriously in the making. Please come back later for updates.\n\n\nBayesian Modeling is the primary choice if you want to obtain the uncertainty associated with the predictions of a model. Of course, there are also voices arguing why Bayesian Deep Learning is a promising avenue.\nThere are some very useful blog entries and notebooks out there (e.g.Â by Thomas Wiecki using Theano PyMC3 and this repo using a more recent version of JAX).\nHowever, those examples only work with the critical assumption that the group sizes are all of the same size. In reality, this is rarely the case, of course.\nHere, I will show you how you can implement a Hierarchical Bayesian Neural Network irrespective of the group sizes you observe in your dataset.\n\n\nCode\nfrom typing import Tuple\nfrom datetime import date\nfrom functools import partial\nfrom warnings import filterwarnings\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\nimport equinox as eqx\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import scale\nimport blackjax\nimport tensorflow_probability.substrates.jax.distributions as tfd\nfrom sklearn.model_selection import train_test_split\n\n\nfilterwarnings(\"ignore\")\n\n\n\n\nCode\nkey = jr.key(int(date.today().strftime(\"%Y%m%d\")))\ncmap = mpl.colormaps[\"PiYG\"]\n\n\nWARNING:2025-06-11 22:20:45,725:jax._src.xla_bridge:791: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\n\n\nCode\n# Data\nn_groups = 18\nn_grps_sq = int(np.sqrt(n_groups))\nn_samples = np.random.randint(10, 200, size=n_groups)\nnoise = 0.3\n\n# MLP params\ndata_dim = 2\nhidden_layer_width = 8\nn_hidden_layers = 3\n\n# Sampling\nnum_warmup = 200\nnum_samples = 400\n\n\n\n\nCode\nX, Y = make_moons(noise=noise, n_samples=1000)\n\nfor i in range(2):\n    plt.scatter(X[Y == i, 0], X[Y == i, 1], color=cmap(float(i)), label=f\"Class {i}\", alpha=0.8)\nplt.legend()\nplt.show()\n\n\n\n\n\nThe standard two-moons dataset plot\n\n\n\n\n\n\nCode\ndef rotate(X, deg):\n    theta = np.radians(deg)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.matrix([[c, -s], [s, c]])\n    X = X.dot(R)\n    return np.asarray(X)\n\n\nnp.random.seed(31)\n\nXs, Ys, gs = [], [], []\nXs_train, Ys_train, gs_train, Xs_test, Ys_test, gs_test = [], [], [], [], [], []\n\nfor i in range(n_groups):\n    # Generate data with 2 classes that are not linearly separable\n    X, Y = make_moons(noise=noise, n_samples=n_samples[i])\n    X = scale(X)\n\n    # Rotate the points randomly for each category\n    rotate_by = np.random.randn() * 90.0\n    X = rotate(X, rotate_by)\n    Xs.append(X)\n    Ys.append(Y)\n    gs.append(X.shape[0])\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=31)\n    Xs_train.append(X_train)\n    Ys_train.append(Y_train)\n    gs_train.append(X_train.shape[0])\n    Xs_test.append(X_test)\n    Ys_test.append(Y_test)\n    gs_test.append(X_test.shape[0])\n\n\n\n\nCode\ndef pad_arrays(arrays, fill_value):\n    max_size = max(array.shape[0] for array in arrays)\n    padded_arrays = []\n    for array in arrays:\n        if array.ndim == 1:\n            padding = (0, max_size - array.shape[0]) \n            padded_array = jnp.pad(array, padding, mode=\"constant\", constant_values=fill_value)\n            padded_arrays.append(padded_array[:, np.newaxis])  \n        else:\n            padding = ((0, max_size - array.shape[0]), (0, 0))  \n            padded_array = jnp.pad(array, padding, mode=\"constant\", constant_values=fill_value)\n            padded_arrays.append(padded_array)\n    return padded_arrays\n\n\n\n\nCode\n# Stack group arrays and create a mask\nfill_value = 1e5\nXs_train = jnp.stack(pad_arrays(Xs_train, fill_value))\nYs_train = jnp.stack(pad_arrays(Ys_train, fill_value)).squeeze(axis=2)\nXs_test = jnp.stack(pad_arrays(Xs_test, fill_value))\nYs_test = jnp.stack(pad_arrays(Ys_test, fill_value)).squeeze(axis=2)\n\nmask_train = jnp.where(Xs_train == fill_value, fill_value, 1)\nmask_test = jnp.where(Xs_test == fill_value, fill_value, 1)\n\n\n\n\nCode\n# Number of rows and columns for subplots\nn_rows = int(np.ceil(n_groups / n_grps_sq))\nn_cols = n_grps_sq\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 10), sharex=True, sharey=True)\n\n# Flatten axes array for easy iteration\naxes = axes.flatten()\n\nfor i, (X, Y, group_size, ax) in enumerate(zip(Xs_train, Ys_train, gs_train, axes)):\n    for c in range(2):\n        ax.scatter(X[Y == c, 0], X[Y == c, 1], color=cmap(float(c)), label=f\"Class {c}\", alpha=0.8)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.legend(frameon=False)\n    ax.set(title=f\"Category {i + 1}, N_training = {group_size}\")\n\n# Hide any unused subplots\nfor j in range(n_groups, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n18 groups with varying numbers of training examples\n\n\n\n\n\n\nCode\ngrid = jnp.mgrid[-3:3:100j, -3:3:100j].reshape((2, -1)).T\ngrid_3d = jnp.repeat(grid[None, ...], n_groups, axis=0)\nmask_grid = jnp.ones(grid_3d.shape)\n\n\n\n\nCode\n@eqx.filter_vmap\ndef make_ensemble(key):\n    # Create an ensemble of models\n    net = eqx.nn.MLP(in_size=data_dim, out_size=1, width_size=hidden_layer_width, depth=n_hidden_layers, key=key)\n    return net\n\n\n@eqx.filter_vmap(in_axes=(eqx.if_array(0), None))\ndef evaluate_ensemble(ensemble, x):\n    # Evaluate each member of the ensemble on the same data\n    o = ensemble(x)\n    return o.mean()\n\n\ndef evaluate_per_ensemble(model, x):\n    return jax.vmap(model)(x)\n\n\ndef apply_ensemble(ensemble, D):\n    # ensemble_fn = partial(evaluate_ensemble, ensemble)\n    preds = eqx.filter_vmap(evaluate_per_ensemble)(ensemble, D)\n    return preds\n\n\nkey = jr.PRNGKey(0)\nhnn = make_ensemble(jr.split(key, n_groups))\n\n\n\n\nCode\nclass NonCentredLinear(eqx.Module):\n    # mu is the global network\n    mu: jax.Array\n    # eps are the n_group local networks\n    eps: jax.Array\n    std: jax.Array\n\n    def __init__(self, in_size, out_size, n_groups, *, key):\n        self.mu = jr.normal(key, (in_size, out_size))\n        self.eps = jr.normal(key, (n_groups, in_size, out_size))\n        self.std = jnp.ones((1,))\n\n    def __call__(self, x):\n        w = self.mu + self.std * self.eps\n        return x @ w\n\n\nclass HNN(eqx.Module):\n    layers: Tuple[NonCentredLinear]\n    out: eqx.nn.Linear\n\n    def __init__(self, layer_width, n_layers, n_groups, *, key):\n        dims = [data_dim] + [layer_width] * n_layers\n        layers = []\n        for n, (_in, _out) in enumerate(zip(dims[:-1], dims[1:])):\n            layer = NonCentredLinear(_in, _out, n_groups, key=jr.fold_in(key, n))\n            layers += [layer]\n        self.layers = tuple(layers)\n        self.out = eqx.nn.Linear(layer_width, 1, key=key)\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n            x = jax.nn.tanh(x)\n        # Vmap over groups and samples\n        o = jax.vmap(jax.vmap(self.out))(x)\n        return o\n\n\n\n\nCode\ndef get_init_apply_fns(model):\n    params, static = eqx.partition(model, eqx.is_inexact_array)\n\n    def init_fn():\n        return params\n\n    def apply_fn(_params, x):\n        model = eqx.combine(_params, static)\n        return model(x)\n\n    return init_fn, apply_fn\n\nhnn = HNN(hidden_layer_width, n_hidden_layers, n_groups, key=key)\ninit_fn, apply_fn = get_init_apply_fns(hnn)\nparams = init_fn()\n\n\n\n\nCode\ndef inference_loop(key, step_fn, initial_state, num_samples):\n    def one_step(state, key):\n        state, _ = step_fn(key, state)\n        return state, state\n\n    keys = jr.split(key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n    return states\n\n\n\n\nCode\ndef get_predictions(model, samples, X, mask, fill_value, key):\n    vectorized_apply = jax.vmap(apply_fn, in_axes=(0, None), out_axes=0)\n    z = vectorized_apply(samples, X)\n    predictions = tfd.Bernoulli(logits=z).sample(seed=key)\n    mask_reshaped = jnp.broadcast_to(\n        jnp.mean(mask, axis=-1).reshape(mask.shape[0], mask.shape[1], 1), predictions.shape\n    )\n    predictions = jnp.where(mask_reshaped == fill_value, jnp.nan, predictions)\n    return predictions.squeeze(-1)\n\n\n\n\nCode\ndef get_mean_predictions(predictions, threshold=0.5):\n    # Compute mean prediction and confidence interval around median\n    mean_prediction = jnp.nanmean(predictions, axis=0)\n    return mean_prediction &gt; threshold\n\n\n\n\nCode\ndef fit_and_eval(\n    key,\n    initial_position,  # Passed from `init_fn` of init/apply function conversion of Equinox NN\n    model,\n    logdensity_fn,\n    X_train,\n    Y_train,\n    mask_train,\n    fill_value,\n    X_test,\n    grid,\n    num_warmup=200,\n    num_samples=100,\n):\n    (\n        warmup_key,\n        inference_key,\n        train_key,\n        test_key,\n        grid_key,\n    ) = jr.split(key, 5)\n\n    # Initialization\n    logprob = partial(logdensity_fn, X=X_train, Y=Y_train, mask=mask_train, fill_value=fill_value, model=model)\n\n    # Warm up\n    adapt = blackjax.window_adaptation(blackjax.nuts, logprob)\n    (final_state, params), _ = adapt.run(warmup_key, initial_position, num_warmup)\n    step_fn = blackjax.nuts(logprob, **params).step\n\n    # Inference\n    states = inference_loop(inference_key, step_fn, final_state, num_samples)\n    samples = states.position\n\n    # Evaluation\n    predictions = get_predictions(model, samples, X_train, mask_train, fill_value, train_key)\n    Y_pred_train = get_mean_predictions(predictions)\n\n    predictions = get_predictions(model, samples, X_test, mask_test, fill_value, test_key)\n    Y_pred_test = get_mean_predictions(predictions)\n\n    pred_grid = get_predictions(model, samples, grid, mask_grid, fill_value, grid_key)\n\n    return Y_pred_train, Y_pred_test, pred_grid\n\n\n\n\nCode\ndef logprior_fn(params):\n    normal = tfd.Normal(0.0, 1.0)\n    leaves, _ = jax.tree_util.tree_flatten(params)\n    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n    return jnp.sum(normal.log_prob(flat_params))\n\ndef logprior_fn_of_hnn(params, model):\n    \"\"\"p(w) where w is NN(X; w)\"\"\"\n    lp = 0.0\n    half_normal = tfd.HalfNormal(1.0)\n    normal = tfd.Normal(0.0, 1.0)\n    for layer in params.layers:\n        lp += normal.log_prob(layer.mu).sum()\n        lp += normal.log_prob(layer.eps).sum()\n        lp += half_normal.log_prob(layer.std).sum()\n    lp += logprior_fn(params.out)\n    return lp\n\n\ndef loglikelihood_fn(params, X, Y, mask, fill_value, model):\n    \"\"\"p(Y|Y_=NN(X; w))\"\"\"\n    logits = jnp.ravel(apply_fn(params, X))\n    # apply the mask: where the mask has the fill value, the logits should also be zero\n    logits = jnp.where(jnp.ravel(mask[:, :, 0]) == fill_value, 0, logits)\n    return jnp.sum(tfd.Bernoulli(logits).log_prob(jnp.ravel(Y)))\n\n\ndef logdensity_fn_of_hnn(params, X, Y, mask, fill_value, model):\n    return logprior_fn_of_hnn(params, model) + loglikelihood_fn(params, X, Y, mask, fill_value, model)\n\n\n\n\nCode\nkey, inference_key = jr.split(key)\n\n(Ys_hierarchical_pred_train, Ys_hierarchical_pred_test, ppc_grid) = fit_and_eval(\n    inference_key,\n    params,\n    hnn,\n    logdensity_fn_of_hnn,\n    Xs_train,\n    Ys_train,\n    mask_train,\n    fill_value,\n    Xs_test,\n    grid_3d,\n    num_warmup=num_warmup,\n    num_samples=num_samples,\n)\n\n\n\n\nCode\ndef reverse_mask(targets, predictions, mask, fill_value):\n    targets, predictions, mask = jnp.ravel(targets), jnp.ravel(predictions), jnp.ravel(mask[:,:,0])\n    positions_to_omit = jnp.where(mask == fill_value)[0]\n    filtered_targets, filtered_predictions = jnp.delete(targets, positions_to_omit), jnp.delete(predictions, positions_to_omit)\n    return filtered_targets,filtered_predictions    \n\n\n\n\nCode\nfiltered_Ys_train, filtered_Ys_hierarchical_pred_train = reverse_mask(\n    Ys_train, Ys_hierarchical_pred_train, mask_train, fill_value\n)\nfiltered_Ys_test, filtered_Ys_hierarchical_pred_test = reverse_mask(\n    Ys_test, Ys_hierarchical_pred_test, mask_test, fill_value\n)\n\n\n\n\nCode\nprint(\"Train accuracy = {:.2f}%\".format(100 * jnp.mean(filtered_Ys_hierarchical_pred_train == filtered_Ys_train)))\n\n\nTrain accuracy = 92.48%\n\n\n\n\nCode\nprint(\"Test accuracy = {:.2f}%\".format(100 * jnp.mean(filtered_Ys_hierarchical_pred_test == filtered_Ys_test)))\n\n\nTest accuracy = 89.02%\n\n\n\n\nCode\ndef plot_decision_surfaces_hierarchical(nrows=2, ncols=2):\n    fig, axes = plt.subplots(figsize=(15, 12), nrows=nrows, ncols=ncols, sharex=True, sharey=True)\n\n    for i, (X, Y_pred, Y_true, ax) in enumerate(zip(Xs_train, Ys_hierarchical_pred_train, Ys_train, axes.flatten())):\n        ax.contourf(\n            grid[:, 0].reshape((100, 100)),\n            grid[:, 1].reshape((100, 100)),\n            ppc_grid[:, i, :].mean(axis=0).reshape(100, 100),\n            cmap=cmap,\n            zorder=0,\n        )\n        for i in range(2):\n            ax.scatter(X[Y_true == i, 0], X[Y_true == i, 1], color=\"w\", alpha=0.8, s=20.0, zorder=1)\n            ax.scatter(\n                X[Y_true == i, 0],\n                X[Y_true == i, 1],\n                color=cmap(float(i)),\n                label=f\"Class {i}\",\n                alpha=0.8,\n                s=10.0,\n                zorder=2,\n            )\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.legend(frameon=False)\n\n\n# %%\nplot_decision_surfaces_hierarchical(nrows=n_grps_sq, ncols=n_grps_sq)\nplt.savefig(\"figures/hbnn_decision_boundaries.png\", bbox_inches=\"tight\")\nplt.show()"
  }
]