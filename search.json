[
  {
    "objectID": "posts/2025-06-11-hierarchical-NN/index.html",
    "href": "posts/2025-06-11-hierarchical-NN/index.html",
    "title": "Hierarchical Bayesian Neural Networks with variable group sizes",
    "section": "",
    "text": "Bayesian Modeling is a very suitable choice if you want to obtain the uncertainty associated with the predictions of a model. Here, typically a Markov Chain Monte Carlo estimator is used, which explores any stationary distribution and recovers (asymptotically) consistent estimators and thus those samplers are of primary interest, because we can essentially (re-)construct any distribution. This can also be the joint distribution of the weights of a Neural Network! This could be incredibly promising, since we can combine the powers from statistical modeling techniques with the of universal function approximation from Neural Networks. To this end, there are recent voices arguing why Bayesian Deep Learning is a promising avenue.\nIn Bayesian Modeling, Hierarchical Bayesian Modeling is a special kind of model specification, helping the sampler to expore the distribution of interest. It is in fact so powerful that once you know about it, you can‚Äôt unsee applications of it (primarily in the Sciences). Hierarchical Modeling can be used if you have some grouped structure in your dataset, e.g.¬†if products can be assigned to clusters that share some properties. More on this technique in Section¬†2.\nThere are some very useful blog entries and notebooks out there (e.g.¬†by Thomas Wiecki using Theano and PyMC3 and this repo using a more recent version of JAX). However, those examples only work with the critical assumption that the group sizes are all of the same size. In reality, this is rarely the case, of course.\nHere, I will show you how you can implement a Hierarchical Bayesian Neural Network irrespective of the group sizes you observe in your dataset.\nThe notebook is structured as follows:\n\n‚úçÔ∏è Create a dataset: Binary classification with unequal observations per group\nüß† What‚Äôs the modelling approach and why does it work?\nüëæ Code the model\nüöÄ Run the model and evaluate\n\n\n1 Setup and dummy data generation\nLet‚Äôs first import the libraries we‚Äôll need:\n\nfrom typing import Tuple\nfrom datetime import date\nfrom functools import partial\nfrom warnings import filterwarnings\nimport jax\nimport jax.random as jr\nimport jax.numpy as jnp\nimport equinox as eqx\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import scale\nimport blackjax\nimport tensorflow_probability.substrates.jax.distributions as tfd\nfrom sklearn.model_selection import train_test_split\n\nfilterwarnings(\"ignore\")\nkey = jr.key(int(date.today().strftime(\"%Y%m%d\")))\ncmap = mpl.colormaps[\"RdYlGn\"]\n\nThoughout the notebook, we‚Äôll use the standard two-moons dataset, being a binary classification problem. Figure¬†1 shows how the dataset looks with some training examples.\n\nnoise = 0.3\nX, Y = make_moons(noise=noise, n_samples=2000)\n\nfor i in range(2):\n    plt.scatter(X[Y == i, 0], X[Y == i, 1], color=cmap(float(i)), label=f\"Class {i}\", alpha=0.8)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†1: The standard two-moons dataset plot\n\n\n\n\n\nNext, let‚Äôs choose some values for the data generation of our grouped dataset. We‚Äôll create several groups with a random number of samples, choose some settings for our Neural Network implementation and set two parameters for the MCMC-Algorithm: the number of ‚Äòwarmup‚Äô samples (which will be discarded after the model fitting finished) and the number of sampling steps.\n\n# Data\nn_groups = 16\nn_grps_sq = int(np.sqrt(n_groups))\nn_samples = np.random.randint(10, 200, size=n_groups)\n\n# MLP params\ndata_dim = 2\nhidden_layer_width = 8\nn_hidden_layers = 3\n\n# Sampling\nnum_warmup = 1000\nnum_samples = 2000\n\nWe then write a function which rotates the dataset in the 2-D space a bit and generate the datasets, store them in lists:\n\ndef rotate(X, deg):\n    theta = np.radians(deg)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.matrix([[c, -s], [s, c]])\n    X = X.dot(R)\n    return np.asarray(X)\n\nnp.random.seed(31)\n\nXs, Ys, gs = [], [], []\nXs_train, Ys_train, gs_train, Xs_test, Ys_test, gs_test = [], [], [], [], [], []\n\nfor i in range(n_groups):\n    # Generate data with 2 classes that are not linearly separable\n    X, Y = make_moons(noise=noise, n_samples=n_samples[i])\n    X = scale(X)\n\n    # Rotate the points randomly for each category\n    rotate_by = np.random.randn() * 90.0\n    X = rotate(X, rotate_by)\n    Xs.append(X)\n    Ys.append(Y)\n    gs.append(X.shape[0])\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=31)\n    Xs_train.append(X_train)\n    Ys_train.append(Y_train)\n    gs_train.append(X_train.shape[0])\n    Xs_test.append(X_test)\n    Ys_test.append(Y_test)\n    gs_test.append(X_test.shape[0])\n\nNext, we pad the entries in our list of datasets such that all the entries have the same shape: the shape of the largest dataset. We also create a mask, marking the elements of the entries which were padded. Padding works here, because we can disregard the masked positions in our datasets in the loglikelihood function.\n\ndef pad_arrays(arrays, fill_value):\n    max_size = max(array.shape[0] for array in arrays)\n    padded_arrays = []\n    for array in arrays:\n        if array.ndim == 1:\n            padding = (0, max_size - array.shape[0]) \n            padded_array = jnp.pad(array, padding, mode=\"constant\", constant_values=fill_value)\n            padded_arrays.append(padded_array[:, np.newaxis])  \n        else:\n            padding = ((0, max_size - array.shape[0]), (0, 0))  \n            padded_array = jnp.pad(array, padding, mode=\"constant\", constant_values=fill_value)\n            padded_arrays.append(padded_array)\n    return padded_arrays\n\n# Stack group arrays and create a mask\nfill_value = 1e5\nXs_train = jnp.stack(pad_arrays(Xs_train, fill_value))\nYs_train = jnp.stack(pad_arrays(Ys_train, fill_value)).squeeze(axis=2)\nXs_test = jnp.stack(pad_arrays(Xs_test, fill_value))\nYs_test = jnp.stack(pad_arrays(Ys_test, fill_value)).squeeze(axis=2)\n\nmask_train = jnp.where(Xs_train == fill_value, fill_value, 1)\nmask_test = jnp.where(Xs_test == fill_value, fill_value, 1)\n\nIn Figure¬†2 you can see how the datasets look and how many entries the individual groups got:\n\n# utility function for plotting\ndef closest_factors(n):\n    a, b = 1, n\n    for i in range(1, int(n**0.5) + 1):\n        if n % i == 0:\n            a, b = i, n // i \n    return a, b\n\n\n# Number of rows and columns for subplots\nn_cols, n_rows = closest_factors(n_groups)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols*3, n_rows*2), sharex=True, sharey=True)\n\n# Flatten axes array for easy iteration\naxes = axes.flatten()\n\nfor i, (X, Y, group_size, ax) in enumerate(zip(Xs_train, Ys_train, gs_train, axes)):\n    for c in range(2):\n        ax.scatter(X[Y == c, 0], X[Y == c, 1], color=cmap(float(c)), label=f\"Class {c}\", alpha=0.8)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.legend(frameon=False)\n    ax.set(title=f\"Category {i + 1}, N_training = {group_size}\")\n\n# Hide any unused subplots\nfor j in range(n_groups, len(axes)):\n    fig.delaxes(axes[j])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigure¬†2: Groups with varying numbers of training examples\n\n\n\n\n\n\n\n2 A Bayesian Hierarchical Neural Network\nNow, let‚Äôs dig into why modeling this dataset with a hierarchical Neural Network might make sense. Beware: we‚Äôll use some vocabulary from statistical modeling.\nThe prominent probabilistic modeler Michael Betancourt provides an in depth introduction to the foundations of hierarchical modeling. Conceptually, hierarchical modeling is an approach if there is a latent population that we can couple context-dependent parameters to. In our dataset, we assume there is some homogeneous structure withtin the groups, whereas the groups may be different between them. This means that we can actually share the weights of the group‚Äôs individual Neural Networks across all networks, because the task (binary classification with some Z-shape) is similar; even though the individual groups are all oriented differently in the 2-D space (difference between the groups).\n\n\n\n\n\n\nWhen does a Hierarchical Bayesian Neural Network make the most sense?\n\n\n\nSummarizing, a HBNN requires that:\n\nyou can make use of a grouping structure in your dataset\nthe data generating process of the individual groups is similar\n\nIt is the strongest, if you have a small-ish number of observations (probably in relation to the difficulty of the learning task?), since in this case ‚Äòtraditional‚Äô approaches will fail (see Tip¬†1).\n\n\n\n\n\n\n\n\nTip¬†1: Recap: Running a separate model per group (example from the Blackjax Sampling book)\n\n\n\n\n\nSince it‚Äôs covered in the HBNN-tutorial from the BlackJax Sampling Book, I will just copy-paste the code here for reference and the curious (and because I don‚Äôt want to loose this example if the Sampling Book disappears at some point).\nThe key takeaway is this: If you fit the models separately, there will be not enough training examples for the Neural Network to capture the nonlinear relationship separating the two classes.\n\nimport matplotlib.pyplot as plt\nimport jax\nfrom datetime import date\n\nfrom functools import partial\nfrom warnings import filterwarnings\n\nfrom flax import linen as nn\nfrom flax.linen.initializers import ones\nimport jax.numpy as jnp\nimport numpy as np\nimport tensorflow_probability.substrates.jax.distributions as tfd\n\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import scale\n\nimport blackjax\n\nfilterwarnings(\"ignore\")\n\nimport matplotlib as mpl\n\nplt.rcParams[\"axes.spines.right\"] = False\nplt.rcParams[\"axes.spines.top\"] = False\n\nrng_key = jax.random.key(int(date.today().strftime(\"%Y%m%d\")))\n\nn_groups_tut = 18\n\nn_grps_sq_tut = int(np.sqrt(n_groups_tut))\nn_samples_tut = 100\n\n\ndef rotate_tut(X, deg):\n    theta = np.radians(deg)\n    c, s = np.cos(theta), np.sin(theta)\n    R = np.matrix([[c, -s], [s, c]])\n\n    X = X.dot(R)\n\n    return np.asarray(X)\n\nnp.random.seed(31)\n\nXs_tut, Ys_tut = [], []\nfor i in range(n_groups_tut):\n    # Generate data with 2 classes that are not linearly separable\n    X, Y = make_moons(noise=0.3, n_samples=n_samples_tut)\n    X = scale(X)\n\n    # Rotate the points randomly for each category\n    rotate_by = np.random.randn() * 90.0\n    X = rotate_tut(X, rotate_by)\n    Xs_tut.append(X)\n    Ys_tut.append(Y)\n\nXs_tut = jnp.stack(Xs_tut)\nYs_tut = jnp.stack(Ys_tut)\n\nXs_tut_train = Xs_tut[:, : n_samples_tut // 2, :]\nXs_tut_test = Xs_tut[:, n_samples_tut // 2 :, :]\nYs_tut_train = Ys_tut[:, : n_samples_tut // 2]\nYs_tut_test = Ys_tut[:, n_samples_tut // 2 :]\n\ngrid = jnp.mgrid[-3:3:100j, -3:3:100j].reshape((2, -1)).T\ngrid_3d = jnp.repeat(grid[None, ...], n_groups_tut, axis=0)\n\ndef inference_loop_tut(rng_key, step_fn, initial_state, num_samples):\n    def one_step(state, rng_key):\n        state, _ = step_fn(rng_key, state)\n        return state, state\n\n    keys = jax.random.split(rng_key, num_samples)\n    _, states = jax.lax.scan(one_step, initial_state, keys)\n\n    return states\n\ndef get_predictions_tut(model, samples, X, rng_key):\n    vectorized_apply = jax.vmap(model.apply, in_axes=(0, None), out_axes=0)\n    z = vectorized_apply(samples, X)\n    predictions = tfd.Bernoulli(logits=z).sample(seed=rng_key)\n\n    return predictions.squeeze(-1)\n\ndef get_mean_predictions_tut(predictions, threshold=0.5):\n    # compute mean prediction and confidence interval around median\n    mean_prediction = jnp.mean(predictions, axis=0)\n    return mean_prediction &gt; threshold\n\ndef fit_and_eval_tut(\n    rng_key,\n    model,\n    logdensity_fn,\n    X_train,\n    Y_train,\n    X_test,\n    grid,\n    n_groups=None,\n    num_warmup=1000,\n    num_samples=2000,\n):\n    (\n        init_key,\n        warmup_key,\n        inference_key,\n        train_key,\n        test_key,\n        grid_key,\n    ) = jax.random.split(rng_key, 6)\n\n    if n_groups is None:\n        initial_position = model.init(init_key, jnp.ones(X_train.shape[-1]))\n    else:\n        initial_position = model.init(init_key, jnp.ones(X_train.shape))\n\n    # initialization\n    logprob = partial(logdensity_fn, X=X_train, Y=Y_train, model=model)\n\n    # warm up\n    adapt = blackjax.window_adaptation(blackjax.nuts, logprob)\n    (final_state, params), _ = adapt.run(warmup_key, initial_position, num_warmup)\n    step_fn = blackjax.nuts(logprob, **params).step\n\n    # inference\n    states = inference_loop_tut(inference_key, step_fn, final_state, num_samples)\n    samples = states.position\n\n    # evaluation\n    predictions = get_predictions_tut(model, samples, X_train, train_key)\n    Y_pred_train = get_mean_predictions_tut(predictions)\n\n    predictions = get_predictions_tut(model, samples, X_test, test_key)\n    Y_pred_test = get_mean_predictions_tut(predictions)\n\n    pred_grid = get_predictions_tut(model, samples, grid, grid_key)\n\n    return Y_pred_train, Y_pred_test, pred_grid\n\n# MLP params\nhidden_layer_width_tut = 5\nn_hidden_layers_tut = 2\n\nclass NN(nn.Module):\n    n_hidden_layers: int\n    layer_width: int\n\n    @nn.compact\n    def __call__(self, x):\n        for i in range(self.n_hidden_layers):\n            x = nn.Dense(features=self.layer_width)(x)\n            x = nn.tanh(x)\n        return nn.Dense(features=1)(x)\n\n\nbnn = NN(n_hidden_layers_tut, hidden_layer_width_tut)\n\ndef logprior_fn_tut(params):\n    leaves, _ = jax.tree_util.tree_flatten(params)\n    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n    return jnp.sum(tfd.Normal(0, 1).log_prob(flat_params))\n\n\ndef loglikelihood_fn_tut(params, X, Y, model):\n    logits = jnp.ravel(model.apply(params, X))\n    return jnp.sum(tfd.Bernoulli(logits).log_prob(Y))\n\n\ndef logdensity_fn_of_bnn_tut(params, X, Y, model):\n    return logprior_fn_tut(params) + loglikelihood_fn_tut(params, X, Y, model)\n\nrng_key, eval_key = jax.random.split(rng_key)\nkeys = jax.random.split(eval_key, n_groups_tut)\n\n\ndef fit_and_eval_single_mlp_tut(key, X_train, Y_train, X_test):\n    return fit_and_eval_tut(\n        key, bnn, logdensity_fn_of_bnn_tut, X_train, Y_train, X_test, grid, n_groups=None\n    )\n\n\nYs_pred_train, Ys_pred_test, ppc_grid_single = jax.vmap(fit_and_eval_single_mlp_tut)(\n    keys, Xs_tut_train, Ys_tut_train, Xs_tut_test\n)\n\ndef plot_decision_surfaces_non_hierarchical_tut(nrows=2, ncols=2):\n    fig, axes = plt.subplots(\n        figsize=(15, 12), nrows=nrows, ncols=ncols, sharex=True, sharey=True\n    )\n    axes = axes.flatten()\n    for i, (X, Y_pred, Y_true, ax) in enumerate(\n        zip(Xs_tut_train, Ys_pred_train, Ys_tut_train, axes)\n    ):\n        ax.contourf(\n            grid[:, 0].reshape(100, 100),\n            grid[:, 1].reshape(100, 100),\n            ppc_grid_single[i, ...].mean(axis=0).reshape(100, 100),\n            cmap=cmap,\n        )\n        for i in range(2):\n            ax.scatter(\n                X[Y_true == i, 0], X[Y_true == i, 1], \n                color=cmap(float(i)), label=f\"Class {i}\", alpha=.8)\n        ax.legend()\n\nplot_decision_surfaces_non_hierarchical_tut(nrows=n_grps_sq_tut, ncols=n_grps_sq_tut)\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 Coding the model\nNow we‚Äôre ready for the modeling code. To get an overview of the model we we‚Äôre about to create, have a look at Figure¬†3. There, we have groups \\(g=1:G\\) with the respective weight matrices \\(w^g_{l}\\) for the input, hidden and output layers \\(l\\).\nThe group weights are drawn from a Normal distribution, which, in a non-centered form means\n\\[\nw^g_l = \\mu_{l} + \\epsilon^g_{l} \\sigma_l\n\\]\nwith\n\\[\n\\begin{align}\\mu_l &\\sim \\mathcal{N}(0,1) \\\\\\epsilon^g_l &\\sim \\mathcal{N}(0,1) \\\\\\sigma_l &= 1\\end{align}\n\\]\nThis non-centered formulation simplifies the space to be explored by our sampler.\n\n\n\n\n\n\nCentered Formulation\n\n\n\nWe don‚Äôt model the individual weights directly. This ‚ÄòCentered Formulation‚Äô would mean \\(w^g_l \\sim \\mathcal{N}(\\mu_l, \\sigma_l)\\) with usually a \\(\\mathcal{N}\\) prior for \\(\\mu\\); and a \\(\\mathcal{N}^+\\) prior for \\(\\sigma\\).\n\n\n\n\n\n\n\n\nFigure¬†3: Graphical Depiction of the model we want to train (slightly adapted from the HBNN sampling book reference)\n\n\n\nIn the following code block, we write the model using Equinox, which in turn uses JAX for all the numerical routines (e.g.¬†autodiff).\n\n1class NonCentredLinear(eqx.Module):\n    mu: jax.Array\n    eps: jax.Array\n    std: jax.Array\n\n    def __init__(self, in_size, out_size, n_groups, *, key):\n2        self.mu = jr.normal(key, (in_size, out_size))\n3        self.eps = jr.normal(key, (n_groups, in_size, out_size))\n4        self.std = jnp.ones((1,))\n\n5    def __call__(self, x):\n        w = self.mu + self.eps * self.std\n        return x @ w\n\n\n6class HNN(eqx.Module):\n    layers: Tuple[NonCentredLinear]\n    out: eqx.nn.Linear\n\n7    def __init__(self, data_dim, layer_width, n_layers, n_groups, *, key):\n        dims = [data_dim] + [layer_width] * n_layers\n        layers = []\n        for n, (_in, _out) in enumerate(zip(dims[:-1], dims[1:])):\n8            layer = NonCentredLinear(_in, _out, n_groups, key=jr.fold_in(key, n))\n            layers += [layer]\n        self.layers = tuple(layers)\n9        self.out = eqx.nn.Linear(layer_width, 1, key=key)\n\n    def __call__(self, x):\n        for layer in self.layers:\n            x = layer(x)\n10            x = jax.nn.tanh(x)\n        # Vmap over groups and samples\n        o = jax.vmap(jax.vmap(self.out))(x)\n        return o\n\n\n1\n\nWrite the Non-centered layers as Equinox module\n\n2\n\nInitialize the weights in \\(\\mu\\) as standard Normal, one for each layer\n\n3\n\nInitialize the weights in \\(\\epsilon\\) as standard Normal, one for each layer and group\n\n4\n\nInitialize \\(\\sigma\\) as 1.\n\n5\n\nNon-centered combination of the matrices and the dot-product of \\(x\\) and \\(w\\).\n\n6\n\nWrite the Hierarchical Neural Network as Equinox module\n\n7\n\nIn this implementation, all layers have the same width\n\n8\n\nCreate all hidden layers\n\n9\n\nFinal linear layer\n\n10\n\nChoose \\(\\tanh\\) as activation function\n\n\n\n\nNext, we instantiate the HNN model and write some code that Equinox needs.\n\ndef get_init_apply_fns(model):\n    params, static = eqx.partition(model, eqx.is_inexact_array)\n\n    def init_fn():\n        return params\n\n    def apply_fn(_params, x):\n        model = eqx.combine(_params, static)\n        return model(x)\n\n    return init_fn, apply_fn\n\n1hnn = HNN(data_dim, hidden_layer_width, n_hidden_layers, n_groups, key=key)\ninit_fn, apply_fn = get_init_apply_fns(hnn)\nparams = init_fn()\n\ndef inference_loop(key, step_fn, initial_state, num_samples):\n    def one_step(state, key):\n        state, _ = step_fn(key, state)\n        return state, state\n\n    keys = jr.split(key, num_samples)\n2    _, states = jax.lax.scan(one_step, initial_state, keys)\n    return states\n\n\n1\n\nInstantiate the HNN model\n\n2\n\nIn Jax, we can use the scan method to iterate over a function\n\n\n\n\nNext, we write the (log)-prior function for the parameters of the model, as well as the log-likelihood.\n\ndef logprior_fn(params):\n    normal = tfd.Normal(0.0, 1.0)\n    leaves, _ = jax.tree_util.tree_flatten(params)\n    flat_params = jnp.concatenate([jnp.ravel(a) for a in leaves])\n    return jnp.sum(normal.log_prob(flat_params))\n\ndef logprior_fn_of_hnn(params, model):\n    \"\"\"p(w) where w is NN(X; w)\"\"\"\n    lp = 0.0\n    half_normal = tfd.HalfNormal(1.0)\n    normal = tfd.Normal(0.0, 1.0)\n    for layer in params.layers:\n        lp += normal.log_prob(layer.mu).sum()\n        lp += normal.log_prob(layer.eps).sum()\n        lp += half_normal.log_prob(layer.std).sum()\n    lp += logprior_fn(params.out)\n    return lp\n\n\ndef loglikelihood_fn(params, X, Y, mask, fill_value, model):\n    \"\"\"p(Y|Y_=NN(X; w))\"\"\"\n    logits = jnp.ravel(apply_fn(params, X))\n1    logits = jnp.where(jnp.ravel(mask[:, :, 0]) == fill_value, 0, logits)\n    return jnp.sum(tfd.Bernoulli(logits).log_prob(jnp.ravel(Y)))\n\n\ndef logdensity_fn_of_hnn(params, X, Y, mask, fill_value, model):\n    return logprior_fn_of_hnn(params, model) + loglikelihood_fn(params, X, Y, mask, fill_value, model)\n\n\n1\n\napply the mask: where the mask has the fill value, the logits should also be zero\n\n\n\n\nAnd some utility functions for extracting the model predictions\n\ndef get_predictions(model, samples, X, mask, fill_value, key):\n    vectorized_apply = jax.vmap(apply_fn, in_axes=(0, None), out_axes=0)\n    z = vectorized_apply(samples, X)\n    predictions = tfd.Bernoulli(logits=z).sample(seed=key)\n    mask_reshaped = jnp.broadcast_to(\n        jnp.mean(mask, axis=-1).reshape(mask.shape[0], mask.shape[1], 1), predictions.shape\n    )\n    predictions = jnp.where(mask_reshaped == fill_value, jnp.nan, predictions)\n    return predictions.squeeze(-1)\n\ndef get_mean_predictions(predictions, threshold=0.5):\n    # Compute mean prediction and confidence interval around median\n    mean_prediction = jnp.nanmean(predictions, axis=0)\n    return mean_prediction &gt; threshold\n\n\n\ndef fit_and_eval(\n    key,\n    initial_position,  # Passed from `init_fn` of init/apply function conversion of Equinox NN\n    model,\n    logdensity_fn,\n    X_train,\n    Y_train,\n    mask_train,\n    fill_value,\n    X_test,\n    grid,\n    num_warmup=20,\n    num_samples=10,\n):\n    (\n        warmup_key,\n        inference_key,\n        train_key,\n        test_key,\n        grid_key,\n    ) = jr.split(key, 5)\n\n    # Initialization\n    logprob = partial(logdensity_fn, X=X_train, Y=Y_train, mask=mask_train, fill_value=fill_value, model=model)\n\n    # Warm up\n    adapt = blackjax.window_adaptation(blackjax.nuts, logprob)\n    (final_state, params), _ = adapt.run(warmup_key, initial_position, num_warmup)\n    step_fn = blackjax.nuts(logprob, **params).step\n\n    # Inference\n    states = inference_loop(inference_key, step_fn, final_state, num_samples)\n    samples = states.position\n\n    # Evaluation\n    predictions = get_predictions(model, samples, X_train, mask_train, fill_value, train_key)\n    Y_pred_train = get_mean_predictions(predictions)\n\n    predictions = get_predictions(model, samples, X_test, mask_test, fill_value, test_key)\n    Y_pred_test = get_mean_predictions(predictions)\n\n    pred_grid = get_predictions(model, samples, grid, mask_grid, fill_value, grid_key)\n\n    return Y_pred_train, Y_pred_test, pred_grid\n\ndef reverse_mask(targets, predictions, mask, fill_value):\n    targets, predictions, mask = jnp.ravel(targets), jnp.ravel(predictions), jnp.ravel(mask[:,:,0])\n    positions_to_omit = jnp.where(mask == fill_value)[0]\n    filtered_targets, filtered_predictions = jnp.delete(targets, positions_to_omit), jnp.delete(predictions, positions_to_omit)\n    return filtered_targets,filtered_predictions\n\nWe create a 3D-grid (n_groups, 100*100, 2) to get the model‚Äôs predictions (with the respective probability) and run the model:\n\ngrid = jnp.mgrid[-3:3:100j, -3:3:100j].reshape((2, -1)).T \ngrid_3d = jnp.repeat(grid[None, ...], n_groups, axis=0)\nmask_grid = jnp.ones(grid_3d.shape)\n\nkey, inference_key = jr.split(key)\n\n(Ys_hierarchical_pred_train, Ys_hierarchical_pred_test, ppc_grid) = fit_and_eval(\n    inference_key,\n    params,\n    hnn,\n    logdensity_fn_of_hnn,\n    Xs_train,\n    Ys_train,\n    mask_train,\n    fill_value,\n    Xs_test,\n    grid_3d,\n    num_warmup=num_warmup,\n    num_samples=num_samples,\n)\n\n\nfiltered_Ys_train, filtered_Ys_hierarchical_pred_train = reverse_mask(\n    Ys_train, Ys_hierarchical_pred_train, mask_train, fill_value\n)\nfiltered_Ys_test, filtered_Ys_hierarchical_pred_test = reverse_mask(\n    Ys_test, Ys_hierarchical_pred_test, mask_test, fill_value\n)\n\n\nprint(\"Train accuracy = {:.2f}%\".format(100 * jnp.mean(filtered_Ys_hierarchical_pred_train == filtered_Ys_train)))\n\nTrain accuracy = 91.91%\n\n\n\nprint(\"Test accuracy = {:.2f}%\".format(100 * jnp.mean(filtered_Ys_hierarchical_pred_test == filtered_Ys_test)))\n\nTest accuracy = 91.89%\n\n\n\ndef plot_decision_surfaces_hierarchical(nrows=2, ncols=2):\n    fig, axes = plt.subplots(figsize=(15, 12), nrows=nrows, ncols=ncols, sharex=True, sharey=True)\n\n    for i, (X, Y_pred, Y_true, ax) in enumerate(zip(Xs_train, Ys_hierarchical_pred_train, Ys_train, axes.flatten())):\n        ax.contourf(\n            grid[:, 0].reshape((100, 100)),\n            grid[:, 1].reshape((100, 100)),\n            ppc_grid[:, i, :].mean(axis=0).reshape(100, 100),\n            cmap=cmap,\n            zorder=0,\n        )\n        for i in range(2):\n            ax.scatter(X[Y_true == i, 0], X[Y_true == i, 1], color=\"w\", alpha=0.8, s=20.0, zorder=1)\n            ax.scatter(\n                X[Y_true == i, 0],\n                X[Y_true == i, 1],\n                color=cmap(float(i)),\n                label=f\"Class {i}\",\n                alpha=0.8,\n                s=10.0,\n                zorder=2,\n            )\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.legend(frameon=False)\n\n\n# %%\nplot_decision_surfaces_hierarchical(nrows=n_grps_sq, ncols=n_grps_sq)\nplt.savefig(\"figures/hbnn_decision_boundaries.png\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4 Summary\nGreat! We successfully wrote a model that can work with varying inpout shapes, by using padding and masking. Written in this way, the Bayesian Hierarchical Neural Network is much more generally applicable compared to an implementation assuming groups of equal sizes.\n\nfrom print_versions import print_versions\nprint_versions(globals())\n\njax==0.6.1\nequinox==0.12.2\nnumpy==2.3.0\nmatplotlib==3.10.3\nblackjax==1.2.5\njaxlib==0.6.1"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This website contains some personal notes and learnings about mathematical and probabilistic modeling, as well as some special cases of machine learning or artificial intelligence.\nThe avatar was created on this free avatar generator."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Hierarchical Bayesian Neural Networks with variable group sizes\n\n\n\nProbabilistic Machine Learning\n\nnonlinear model\n\nHierarchical Modeling\n\njax\n\nblackjax\n\n\n\nStandard implementations and publically available tutorials of Hierarchical Bayesian Neural Networks (HBNNs) lack the ability to work with varying numbers of training examples per group. In this entry, I will show you how we can make use of padding and masking in order to be able to train HBNNs.\n\n\n\n\n\nJun 13, 2025\n\n\nJulius Mehringer\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJun 1, 2025\n\n\nJulius Mehringer\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-06-11-welcome/index.html",
    "href": "posts/2025-06-11-welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Hi! Glad your‚Äôre here! üôÇ\nIt‚Äôs time to step into the blog-o-sphere. I‚Äôve learnt so much from other peoples‚Äô blogs and would find great joy of someone else would learn something from my learnings as well.\nFor now, this is it on here. I will add some content from time to time.\nCheers!"
  }
]